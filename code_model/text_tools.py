"""
æ–‡æœ¬å¤„ç†å·¥å…·ç±»
åŒ…å«æŸ¥æ‰¾æ›¿æ¢ã€è¯é¢‘ç»Ÿè®¡ã€æ–‡æœ¬æ‘˜è¦ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import re
import string
import os
from collections import Counter
from typing import List, Dict, Tuple, Optional

# é«˜çº§è‡ªç„¶è¯­è¨€å¤„ç†åº“
try:
    import spacy
    SPACY_AVAILABLE = True
except (ImportError, ValueError) as e:
    SPACY_AVAILABLE = False
    print(f"spaCyä¸å¯ç”¨: {e}")

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except (ImportError, ValueError) as e:
    TEXTBLOB_AVAILABLE = False
    print(f"TextBlobä¸å¯ç”¨: {e}")

try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except (ImportError, ValueError) as e:
    VADER_AVAILABLE = False
    print(f"VADERä¸å¯ç”¨: {e}")

try:
    import stanza
    STANZA_AVAILABLE = True
except (ImportError, ValueError) as e:
    STANZA_AVAILABLE = False
    print(f"Stanzaä¸å¯ç”¨: {e}")

# æ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†æåº“
try:
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
    import torch
    TRANSFORMERS_AVAILABLE = True
except (ImportError, ValueError) as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"Transformersä¸å¯ç”¨: {e}")

try:
    from snownlp import SnowNLP
    SNOWNLP_AVAILABLE = True
except (ImportError, ValueError) as e:
    SNOWNLP_AVAILABLE = False
    print(f"SnowNLPä¸å¯ç”¨: {e}")

# ä¸­æ–‡åˆ†è¯åº“
try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
except (ImportError, ValueError) as e:
    JIEBA_AVAILABLE = False
    print(f"jiebaä¸å¯ç”¨: {e}")

try:
    import pkuseg
    PKUSEG_AVAILABLE = True
    print("âœ“ pkusegå¯ç”¨")
except (ImportError, ValueError) as e:
    PKUSEG_AVAILABLE = False
    print(f"pkusegä¸å¯ç”¨: {e}")

try:
    import thulac
    THULAC_AVAILABLE = True
    print("âœ“ thulacå¯ç”¨")
except (ImportError, ValueError) as e:
    THULAC_AVAILABLE = False
    print(f"thulacä¸å¯ç”¨: {e}")

# TextTeaseræ‘˜è¦åº“ï¼ˆç”±äºä¾èµ–é—®é¢˜ï¼Œæˆ‘ä»¬å®ç°è‡ªå·±çš„è½»é‡çº§ç‰ˆæœ¬ï¼‰
# try:
#     from textteaser import TextTeaser
#     TEXTTEASER_AVAILABLE = True
#     print("âœ“ TextTeaserå¯ç”¨")
# except (ImportError, ValueError) as e:
#     TEXTTEASER_AVAILABLE = False
#     print(f"TextTeaserä¸å¯ç”¨: {e}")

# æˆ‘ä»¬å®ç°è‡ªå·±çš„TextTeaseré£æ ¼ç®—æ³•
TEXTTEASER_AVAILABLE = True
print("âœ“ è½»é‡çº§TextTeaserç®—æ³•å¯ç”¨")

# Qwen3å¤§æ¨¡å‹æ‘˜è¦
try:
    import requests
    import json
    QWEN3_AVAILABLE = True
    print("âœ“ Requestsåº“å¯ç”¨ï¼Œæ”¯æŒQwen3æ‘˜è¦")
except (ImportError, ValueError) as e:
    QWEN3_AVAILABLE = False
    print(f"Requestsåº“ä¸å¯ç”¨: {e}")

# å¯¼å…¥åœç”¨è¯ç®¡ç†å™¨
try:
    from .stopwords import StopwordsManager
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from stopwords import StopwordsManager
    except ImportError:
        print("åœç”¨è¯ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€åŠŸèƒ½")


class TextProcessor:
    """æ–‡æœ¬å¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self):
        self.text = ""
        self.original_text = ""

        # åˆå§‹åŒ–åœç”¨è¯ç®¡ç†å™¨
        try:
            self.stopwords_manager = StopwordsManager()
        except NameError:
            self.stopwords_manager = None
            print("åœç”¨è¯ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨åœç”¨è¯è¿‡æ»¤")

        # åˆå§‹åŒ–NLPæ¨¡å‹å’Œåˆ†è¯å™¨
        self.nlp_models = {}
        self.segmenters = {}
        self._init_nlp_models()
        self._init_segmenters()

        # åˆå§‹åŒ–TextTeaser
        self.textteaser = None
        self._init_textteaser()

        # åˆå§‹åŒ–Qwen3å®¢æˆ·ç«¯
        self.qwen3_client = None
        self._init_qwen3()


    
    def load_text(self, text: str) -> None:
        """åŠ è½½æ–‡æœ¬"""
        self.text = text
        self.original_text = text
    
    def load_from_file(self, file_path: str, encoding: str = 'utf-8') -> None:
        """ä»æ–‡ä»¶åŠ è½½æ–‡æœ¬"""
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                self.load_text(f.read())
        except FileNotFoundError:
            raise FileNotFoundError(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        except UnicodeDecodeError:
            raise UnicodeDecodeError(f"æ— æ³•ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
    
    def save_to_file(self, file_path: str, encoding: str = 'utf-8') -> None:
        """ä¿å­˜æ–‡æœ¬åˆ°æ–‡ä»¶"""
        with open(file_path, 'w', encoding=encoding) as f:
            f.write(self.text)
    
    def find_and_replace(self, pattern: str, replacement: str, 
                        use_regex: bool = False, case_sensitive: bool = True) -> Tuple[str, int]:
        """
        æŸ¥æ‰¾å’Œæ›¿æ¢æ–‡æœ¬
        
        Args:
            pattern: è¦æŸ¥æ‰¾çš„æ¨¡å¼
            replacement: æ›¿æ¢æ–‡æœ¬
            use_regex: æ˜¯å¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            case_sensitive: æ˜¯å¦åŒºåˆ†å¤§å°å†™
            
        Returns:
            (æ›¿æ¢åçš„æ–‡æœ¬, æ›¿æ¢æ¬¡æ•°)
        """
        flags = 0 if case_sensitive else re.IGNORECASE
        
        if use_regex:
            try:
                new_text, count = re.subn(pattern, replacement, self.text, flags=flags)
            except re.error as e:
                raise ValueError(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")
        else:
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ç”¨äºå­—é¢åŒ¹é…
            escaped_pattern = re.escape(pattern)
            new_text, count = re.subn(escaped_pattern, replacement, self.text, flags=flags)
        
        self.text = new_text
        return new_text, count
    
    def find_matches(self, pattern: str, use_regex: bool = False, 
                    case_sensitive: bool = True) -> List[Tuple[int, str]]:
        """
        æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
        
        Returns:
            [(ä½ç½®, åŒ¹é…æ–‡æœ¬), ...]
        """
        flags = 0 if case_sensitive else re.IGNORECASE
        
        if use_regex:
            try:
                matches = [(m.start(), m.group()) for m in re.finditer(pattern, self.text, flags=flags)]
            except re.error as e:
                raise ValueError(f"æ­£åˆ™è¡¨è¾¾å¼é”™è¯¯: {e}")
        else:
            escaped_pattern = re.escape(pattern)
            matches = [(m.start(), m.group()) for m in re.finditer(escaped_pattern, self.text, flags=flags)]
        
        return matches
    
    def word_frequency(self, ignore_case: bool = True,
                      min_word_length: int = 1,
                      exclude_punctuation: bool = True,
                      segmentation_method: str = 'auto',
                      exclude_stopwords: bool = True,
                      exclude_numbers: bool = True,
                      exclude_single_chars: bool = True) -> Dict[str, int]:
        """
        ç»Ÿè®¡è¯é¢‘ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†è¯ï¼‰

        Args:
            ignore_case: æ˜¯å¦å¿½ç•¥å¤§å°å†™
            min_word_length: æœ€å°è¯é•¿
            exclude_punctuation: æ˜¯å¦æ’é™¤æ ‡ç‚¹ç¬¦å·
            segmentation_method: åˆ†è¯æ–¹æ³• ('auto', 'jieba', 'pkuseg', 'thulac', 'basic')
            exclude_stopwords: æ˜¯å¦æ’é™¤åœç”¨è¯
            exclude_numbers: æ˜¯å¦æ’é™¤çº¯æ•°å­—
            exclude_single_chars: æ˜¯å¦æ’é™¤å•å­—ç¬¦

        Returns:
            {è¯: é¢‘ç‡}
        """
        if not self.text.strip():
            return {}
        text = self.text

        if ignore_case:
            text = text.lower()

        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œæ¢è¡Œç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼
        if exclude_punctuation:
            # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            chinese_punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€â€œâ€â€¦â€¦'
            # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
            english_punctuation = string.punctuation
            all_punctuation = chinese_punctuation + english_punctuation

            # å°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œè€Œä¸æ˜¯ç›´æ¥åˆ é™¤
            for punct in all_punctuation:
                text = text.replace(punct, ' ')

        # å°†æ¢è¡Œç¬¦ä¹Ÿæ›¿æ¢ä¸ºç©ºæ ¼
        text = text.replace('\n', ' ').replace('\r', ' ')

        # ä½¿ç”¨æ™ºèƒ½åˆ†è¯æ›¿ä»£ç®€å•åˆ†å‰²
        segments = self.segment_text(
            text=text,
            method=segmentation_method,
            mode='accurate',
            with_pos=False
        )

        # æå–è¯æ±‡
        words = [seg['word'] for seg in segments]

        # è¿‡æ»¤æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if exclude_punctuation:
            # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
            chinese_punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€""â€¦â€¦'
            # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
            english_punctuation = string.punctuation
            all_punctuation = chinese_punctuation + english_punctuation

            # è¿‡æ»¤çº¯æ ‡ç‚¹ç¬¦å·çš„è¯
            words = [word for word in words if not all(c in all_punctuation for c in word)]

        # è¿‡æ»¤çŸ­è¯å’Œç©ºè¯
        words = [word for word in words if word.strip() and len(word) >= min_word_length]

        # è¿‡æ»¤çº¯æ•°å­—
        if exclude_numbers:
            words = [word for word in words if not word.isdigit()]

        # è¿‡æ»¤å•å­—ç¬¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if exclude_single_chars:
            words = [word for word in words if len(word) > 1]

        # è¿‡æ»¤åœç”¨è¯
        if exclude_stopwords and self.stopwords_manager:
            words = self.stopwords_manager.filter_stopwords(words)

        # ç»Ÿè®¡é¢‘ç‡
        word_count = Counter(words)

        return dict(word_count)
    
    def get_top_words(self, n: int = 10, **kwargs) -> List[Tuple[str, int]]:
        """è·å–å‡ºç°é¢‘ç‡æœ€é«˜çš„nä¸ªè¯"""
        word_freq = self.word_frequency(**kwargs)
        return Counter(word_freq).most_common(n)
    
    def generate_summary(self, num_sentences: int = 3,
                        method: str = 'frequency', title: str = '') -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦

        Args:
            num_sentences: æ‘˜è¦å¥å­æ•°
            method: æ‘˜è¦æ–¹æ³• ('frequency', 'position', 'hybrid', 'textteaser', 'qwen3')
            title: æ–‡æœ¬æ ‡é¢˜ï¼ˆTextTeaserå’ŒQwen3éœ€è¦ï¼‰

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        sentences = self._split_sentences(self.text)

        if len(sentences) <= num_sentences:
            return self.text

        if method == 'frequency':
            return self._frequency_based_summary(sentences, num_sentences)
        elif method == 'position':
            return self._position_based_summary(sentences, num_sentences)
        elif method == 'hybrid':
            return self._hybrid_summary(sentences, num_sentences)
        elif method == 'textteaser':
            return self._textteaser_summary(title, num_sentences)
        elif method == 'qwen3':
            return self._qwen3_summary(title, num_sentences)
        else:
            raise ValueError(f"æœªçŸ¥çš„æ‘˜è¦æ–¹æ³•: {method}")
    
    def _split_sentences(self, text: str) -> List[str]:
        """åˆ†å‰²å¥å­"""
        # æ”¹è¿›çš„å¥å­åˆ†å‰²ï¼ˆæ”¯æŒä¸­æ–‡æ ‡ç‚¹ç¬¦å·ï¼‰
        # æŒ‰æ®µè½åˆ†å‰²ï¼Œç„¶åæŒ‰å¥å­åˆ†å‰²
        paragraphs = text.split('\n\n')
        sentences = []

        for paragraph in paragraphs:
            if paragraph.strip():
                # æŒ‰ä¸­æ–‡å’Œè‹±æ–‡å¥å·åˆ†å‰²
                para_sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', paragraph)
                for sent in para_sentences:
                    sent = sent.strip()
                    if sent and len(sent) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„å¥å­
                        sentences.append(sent)

        return sentences
    
    def _frequency_based_summary(self, sentences: List[str], num_sentences: int) -> str:
        """åŸºäºè¯é¢‘çš„æ‘˜è¦"""
        # è®¡ç®—è¯é¢‘
        word_freq = self.word_frequency(ignore_case=True, exclude_punctuation=True)
        
        # è®¡ç®—å¥å­å¾—åˆ†
        sentence_scores = []
        for sentence in sentences:
            words = sentence.lower().translate(str.maketrans('', '', string.punctuation)).split()
            score = sum(word_freq.get(word, 0) for word in words)
            sentence_scores.append((score, sentence))
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
        sentence_scores.sort(reverse=True)
        top_sentences = [sent for _, sent in sentence_scores[:num_sentences]]
        
        return '. '.join(top_sentences) + '.'
    
    def _position_based_summary(self, sentences: List[str], num_sentences: int) -> str:
        """åŸºäºä½ç½®çš„æ‘˜è¦ï¼ˆé€‰æ‹©å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾çš„å¥å­ï¼‰"""
        total = len(sentences)
        indices = []
        
        if num_sentences >= 1:
            indices.append(0)  # ç¬¬ä¸€å¥
        if num_sentences >= 2:
            indices.append(total - 1)  # æœ€åä¸€å¥
        if num_sentences >= 3:
            indices.append(total // 2)  # ä¸­é—´å¥å­
        
        # å¦‚æœéœ€è¦æ›´å¤šå¥å­ï¼Œå‡åŒ€åˆ†å¸ƒ
        while len(indices) < num_sentences and len(indices) < total:
            for i in range(1, total - 1):
                if i not in indices:
                    indices.append(i)
                    if len(indices) >= num_sentences:
                        break
        
        indices.sort()
        selected_sentences = [sentences[i] for i in indices]
        return '. '.join(selected_sentences) + '.'
    
    def _hybrid_summary(self, sentences: List[str], num_sentences: int) -> str:
        """æ··åˆæ–¹æ³•æ‘˜è¦"""
        # ç»“åˆè¯é¢‘å’Œä½ç½®æƒé‡
        word_freq = self.word_frequency(ignore_case=True, exclude_punctuation=True)
        total_sentences = len(sentences)
        
        sentence_scores = []
        for i, sentence in enumerate(sentences):
            words = sentence.lower().translate(str.maketrans('', '', string.punctuation)).split()
            freq_score = sum(word_freq.get(word, 0) for word in words)
            
            # ä½ç½®æƒé‡ï¼šå¼€å¤´å’Œç»“å°¾å¥å­æƒé‡æ›´é«˜
            if i == 0 or i == total_sentences - 1:
                position_weight = 1.5
            elif i < total_sentences * 0.2 or i > total_sentences * 0.8:
                position_weight = 1.2
            else:
                position_weight = 1.0
            
            final_score = freq_score * position_weight
            sentence_scores.append((final_score, sentence))
        
        sentence_scores.sort(reverse=True)
        top_sentences = [sent for _, sent in sentence_scores[:num_sentences]]
        
        return '. '.join(top_sentences) + '.'

    def _textteaser_summary(self, title: str = '', num_sentences: int = 3) -> str:
        """ä½¿ç”¨TextTeaseré£æ ¼ç®—æ³•ç”Ÿæˆæ‘˜è¦"""
        if not self.textteaser:
            # å¦‚æœTextTeaserä¸å¯ç”¨ï¼Œé™çº§åˆ°æ··åˆæ–¹æ³•
            print("TextTeaserä¸å¯ç”¨ï¼Œä½¿ç”¨æ··åˆæ–¹æ³•æ›¿ä»£")
            return self._hybrid_summary(self._split_sentences(self.text), num_sentences)

        try:
            sentences = self._split_sentences(self.text)

            if len(sentences) <= num_sentences:
                return self.text

            # å¦‚æœæ²¡æœ‰æä¾›æ ‡é¢˜ï¼Œå°è¯•ä»æ–‡æœ¬ç¬¬ä¸€å¥æå–
            if not title:
                if sentences:
                    title = sentences[0][:50] + "..." if len(sentences[0]) > 50 else sentences[0]
                else:
                    title = "æ–‡æœ¬æ‘˜è¦"

            # ä½¿ç”¨TextTeaseré£æ ¼çš„è¯„åˆ†ç®—æ³•
            sentence_scores = self._calculate_textteaser_scores(sentences, title)

            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å¥å­
            sentence_scores.sort(reverse=True)
            top_sentences = [sent for _, sent in sentence_scores[:num_sentences]]

            return '. '.join(top_sentences) + '.'

        except Exception as e:
            print(f"TextTeaseræ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°æ··åˆæ–¹æ³•
            return self._hybrid_summary(self._split_sentences(self.text), num_sentences)

    def _calculate_textteaser_scores(self, sentences: List[str], title: str) -> List[Tuple[float, str]]:
        """
        è®¡ç®—TextTeaseré£æ ¼çš„å¥å­è¯„åˆ†

        åŸºäºä»¥ä¸‹ç‰¹å¾ï¼š
        1. æ ‡é¢˜ç›¸ä¼¼åº¦ (Title Similarity)
        2. å¥å­ä½ç½® (Sentence Position)
        3. å¥å­é•¿åº¦ (Sentence Length)
        4. å…³é”®è¯é¢‘ç‡ (Keyword Frequency)
        """
        sentence_scores = []
        total_sentences = len(sentences)

        # è®¡ç®—è¯é¢‘ç”¨äºå…³é”®è¯è¯„åˆ†
        word_freq = self.word_frequency(ignore_case=True, exclude_punctuation=True)

        # é¢„å¤„ç†æ ‡é¢˜ï¼Œæå–å…³é”®è¯
        title_words = self._extract_keywords(title.lower())

        for i, sentence in enumerate(sentences):
            # 1. æ ‡é¢˜ç›¸ä¼¼åº¦è¯„åˆ† (0-1)
            title_score = self._calculate_title_similarity(sentence.lower(), title_words)

            # 2. ä½ç½®è¯„åˆ† (0-1)
            position_score = self._calculate_position_score(i, total_sentences)

            # 3. é•¿åº¦è¯„åˆ† (0-1)
            length_score = self._calculate_length_score(sentence)

            # 4. å…³é”®è¯é¢‘ç‡è¯„åˆ† (0-1)
            keyword_score = self._calculate_keyword_score(sentence, word_freq)

            # TextTeaseré£æ ¼çš„ç»¼åˆè¯„åˆ†
            # å„ç‰¹å¾æƒé‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦(40%), ä½ç½®(20%), é•¿åº¦(15%), å…³é”®è¯(25%)
            # æé«˜æ ‡é¢˜ç›¸ä¼¼åº¦çš„æƒé‡ï¼Œä½¿å…¶å¯¹æ‘˜è¦ç»“æœå½±å“æ›´å¤§
            final_score = (
                title_score * 0.40 +
                position_score * 0.20 +
                length_score * 0.15 +
                keyword_score * 0.25
            )

            sentence_scores.append((final_score, sentence))

        return sentence_scores

    def _extract_keywords(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        import string
        text = text.translate(str.maketrans('', '', string.punctuation))

        # åˆ†è¯
        words = text.split()

        # è¿‡æ»¤åœç”¨è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.stopwords_manager:
            words = self.stopwords_manager.filter_stopwords(words)

        # è¿‡æ»¤çŸ­è¯
        words = [word for word in words if len(word) > 2]

        return words

    def _calculate_title_similarity(self, sentence: str, title_words: List[str]) -> float:
        """è®¡ç®—å¥å­ä¸æ ‡é¢˜çš„ç›¸ä¼¼åº¦"""
        if not title_words:
            return 0.0

        sentence_words = self._extract_keywords(sentence)
        if not sentence_words:
            return 0.0

        # è®¡ç®—äº¤é›†
        common_words = set(sentence_words) & set(title_words)

        if not common_words:
            return 0.0

        # æ”¹è¿›çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šç»“åˆJaccardç›¸ä¼¼åº¦å’Œè¯æ±‡è¦†ç›–ç‡
        jaccard_similarity = len(common_words) / len(set(sentence_words) | set(title_words))
        title_coverage = len(common_words) / len(title_words)  # æ ‡é¢˜è¯æ±‡è¦†ç›–ç‡
        sentence_coverage = len(common_words) / len(sentence_words)  # å¥å­è¯æ±‡è¦†ç›–ç‡

        # ç»¼åˆè¯„åˆ†ï¼šJaccardç›¸ä¼¼åº¦(40%) + æ ‡é¢˜è¦†ç›–ç‡(40%) + å¥å­è¦†ç›–ç‡(20%)
        similarity = (jaccard_similarity * 0.4 + title_coverage * 0.4 + sentence_coverage * 0.2)

        # å¦‚æœæœ‰å¤šä¸ªåŒ¹é…è¯ï¼Œç»™äºˆé¢å¤–å¥–åŠ±
        if len(common_words) > 1:
            similarity *= 1.2

        return min(similarity, 1.0)

    def _calculate_position_score(self, position: int, total_sentences: int) -> float:
        """è®¡ç®—ä½ç½®è¯„åˆ†"""
        if total_sentences <= 1:
            return 1.0

        # TextTeaseré£æ ¼ï¼šå¼€å¤´å’Œç»“å°¾å¥å­å¾—åˆ†æ›´é«˜
        relative_position = position / (total_sentences - 1)

        if position == 0:  # ç¬¬ä¸€å¥
            return 1.0
        elif position == total_sentences - 1:  # æœ€åä¸€å¥
            return 0.8
        elif relative_position <= 0.1:  # å‰10%
            return 0.9
        elif relative_position >= 0.9:  # å10%
            return 0.7
        else:  # ä¸­é—´éƒ¨åˆ†
            return 0.3

    def _calculate_length_score(self, sentence: str) -> float:
        """è®¡ç®—é•¿åº¦è¯„åˆ†"""
        words = sentence.split()
        word_count = len(words)

        # TextTeaseré£æ ¼ï¼šç†æƒ³é•¿åº¦ä¸º15-25ä¸ªè¯
        if 15 <= word_count <= 25:
            return 1.0
        elif 10 <= word_count <= 30:
            return 0.8
        elif 5 <= word_count <= 35:
            return 0.6
        elif word_count < 5:
            return 0.2  # å¤ªçŸ­
        else:
            return 0.4  # å¤ªé•¿

    def _calculate_keyword_score(self, sentence: str, word_freq: Dict[str, int]) -> float:
        """è®¡ç®—å…³é”®è¯è¯„åˆ†"""
        if not word_freq:
            return 0.0

        sentence_words = self._extract_keywords(sentence.lower())
        if not sentence_words:
            return 0.0

        # è®¡ç®—å¥å­ä¸­é«˜é¢‘è¯çš„å¯†åº¦
        total_freq = sum(word_freq.values())
        sentence_freq_sum = sum(word_freq.get(word, 0) for word in sentence_words)

        if total_freq == 0:
            return 0.0

        # å½’ä¸€åŒ–è¯„åˆ†
        score = (sentence_freq_sum / total_freq) * len(sentence_words)
        return min(score, 1.0)

    def _qwen3_summary(self, title: str = '', num_sentences: int = 3) -> str:
        """ä½¿ç”¨Qwen3å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦"""
        if not self.qwen3_client:
            # å¦‚æœQwen3ä¸å¯ç”¨ï¼Œé™çº§åˆ°TextTeaseræ–¹æ³•
            print("Qwen3æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨TextTeaseræ–¹æ³•æ›¿ä»£")
            return self._textteaser_summary(title, num_sentences)

        try:
            # æ„å»ºæç¤ºè¯
            if title:
                content = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. æ‘˜è¦åº”è¯¥åŒ…å«{num_sentences}ä¸ªå¥å­
2. æ‘˜è¦åº”è¯¥å›´ç»•æ ‡é¢˜"{title}"çš„ä¸»é¢˜
3. æ‘˜è¦åº”è¯¥å‡†ç¡®æ¦‚æ‹¬æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹
4. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€
5. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜

æ–‡æœ¬å†…å®¹ï¼š
{self.text}

æ‘˜è¦ï¼š"""
            else:
                content = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç”Ÿæˆæ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. æ‘˜è¦åº”è¯¥åŒ…å«{num_sentences}ä¸ªå¥å­
2. æ‘˜è¦åº”è¯¥å‡†ç¡®æ¦‚æ‹¬æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹
3. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€
4. ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜

æ–‡æœ¬å†…å®¹ï¼š
{self.text}

æ‘˜è¦ï¼š"""

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.qwen3_model,
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "stream": False
            }

            # è°ƒç”¨Qwen3æ¨¡å‹API
            headers = {'Content-type': 'application/json'}
            response = requests.post(self.qwen3_api_url,
                                   data=json.dumps(data),
                                   headers=headers,
                                   timeout=60)

            if response.status_code == 200:
                response_data = response.json()
                summary = response_data["message"]["content"].strip()

                # é«˜çº§åå¤„ç†ï¼šæ¸…ç†Qwen3è¾“å‡º
                summary = self._clean_qwen3_output(summary)

                return summary
            else:
                print(f"Qwen3 APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return self._textteaser_summary(title, num_sentences)

        except Exception as e:
            print(f"Qwen3æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            # é™çº§åˆ°TextTeaseræ–¹æ³•
            return self._textteaser_summary(title, num_sentences)

    def _clean_qwen3_output(self, text: str) -> str:
        """æ¸…ç†Qwen3æ¨¡å‹è¾“å‡ºï¼Œç§»é™¤thinkæ ‡ç­¾å’Œæ ¼å¼åŒ–å†…å®¹"""
        import re

        # ç§»é™¤<think>...</think>æ ‡ç­¾åŠå…¶å†…å®¹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)

        # ç§»é™¤å¯èƒ½çš„å‰ç¼€æ ‡è®°
        prefixes_to_remove = [
            'æ‘˜è¦ï¼š', '**æ‘˜è¦ï¼š**', '**æ‘˜è¦**ï¼š',
            'æ€»ç»“ï¼š', '**æ€»ç»“ï¼š**', '**æ€»ç»“**ï¼š',
            'æ¦‚è¦ï¼š', '**æ¦‚è¦ï¼š**', '**æ¦‚è¦**ï¼š'
        ]

        for prefix in prefixes_to_remove:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()
                break

        # ç§»é™¤Markdownæ ¼å¼æ ‡è®°
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # ç§»é™¤ç²—ä½“æ ‡è®°
        text = re.sub(r'\*(.*?)\*', r'\1', text)      # ç§»é™¤æ–œä½“æ ‡è®°
        text = re.sub(r'`(.*?)`', r'\1', text)        # ç§»é™¤ä»£ç æ ‡è®°

        # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
        text = re.sub(r'\n\s*\n', '\n', text)  # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'^\s+|\s+$', '', text)  # ç§»é™¤é¦–å°¾ç©ºç™½

        # ç¡®ä¿å¥å­ä¹‹é—´æœ‰é€‚å½“çš„åˆ†éš”
        text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([^ã€‚ï¼ï¼Ÿ\s])', r'\1 \2', text)

        return text.strip()
    
    def reset_text(self) -> None:
        """é‡ç½®æ–‡æœ¬åˆ°åŸå§‹çŠ¶æ€"""
        self.text = self.original_text
    
    def get_text_stats(self) -> Dict[str, int]:
        """è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'å­—ç¬¦æ•°': len(self.text),
            'è¯æ•°': len(self.text.split()),
            'å¥å­æ•°': len(self._split_sentences(self.text)),
            'æ®µè½æ•°': len([p for p in self.text.split('\n\n') if p.strip()])
        }

    def add_custom_stopwords(self, words):
        """æ·»åŠ è‡ªå®šä¹‰åœç”¨è¯"""
        if self.stopwords_manager:
            self.stopwords_manager.add_custom_stopwords(words)
        else:
            print("åœç”¨è¯ç®¡ç†å™¨ä¸å¯ç”¨")

    def remove_custom_stopwords(self, words):
        """ç§»é™¤è‡ªå®šä¹‰åœç”¨è¯"""
        if self.stopwords_manager:
            self.stopwords_manager.remove_custom_stopwords(words)
        else:
            print("åœç”¨è¯ç®¡ç†å™¨ä¸å¯ç”¨")

    def clear_custom_stopwords(self):
        """æ¸…ç©ºè‡ªå®šä¹‰åœç”¨è¯"""
        if self.stopwords_manager:
            self.stopwords_manager.clear_custom_stopwords()
        else:
            print("åœç”¨è¯ç®¡ç†å™¨ä¸å¯ç”¨")

    def get_custom_stopwords(self):
        """è·å–è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨"""
        if self.stopwords_manager:
            return self.stopwords_manager.get_custom_stopwords()
        else:
            return []

    def _init_nlp_models(self):
        """åˆå§‹åŒ–NLPæ¨¡å‹"""
        # åˆå§‹åŒ–spaCyæ¨¡å‹
        if SPACY_AVAILABLE:
            try:
                # å°è¯•åŠ è½½ä¸­æ–‡æ¨¡å‹
                self.nlp_models['spacy_zh'] = spacy.load("zh_core_web_sm")
                print("âœ“ spaCyä¸­æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"spaCyä¸­æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                try:
                    # å¦‚æœä¸­æ–‡æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•è‹±æ–‡æ¨¡å‹
                    self.nlp_models['spacy_en'] = spacy.load("en_core_web_sm")
                    print("âœ“ spaCyè‹±æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    print(f"spaCyè‹±æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")

        # åˆå§‹åŒ–VADERæƒ…æ„Ÿåˆ†æå™¨
        if VADER_AVAILABLE:
            try:
                self.nlp_models['vader'] = SentimentIntensityAnalyzer()
                print("âœ“ VADERæƒ…æ„Ÿåˆ†æå™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"VADERåŠ è½½å¤±è´¥: {e}")

        # åˆå§‹åŒ–Stanzaæ¨¡å‹ï¼ˆç¦»çº¿ä¼˜å…ˆæ¨¡å¼ï¼‰
        if STANZA_AVAILABLE:
            try:
                # å°è¯•åˆå§‹åŒ–ä¸­æ–‡æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
                self.nlp_models['stanza_zh'] = stanza.Pipeline(
                    'zh-hans',
                    processors='tokenize,pos,lemma,depparse',
                    download_method=stanza.DownloadMethod.REUSE_RESOURCES,
                    verbose=False,
                    use_gpu=False
                )
                print("âœ“ Stanzaä¸­æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"Stanzaä¸­æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                try:
                    # å¦‚æœä¸­æ–‡æ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•è‹±æ–‡æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
                    self.nlp_models['stanza_en'] = stanza.Pipeline(
                        'en',
                        processors='tokenize,pos,lemma,depparse',
                        download_method=stanza.DownloadMethod.REUSE_RESOURCES,
                        verbose=False,
                        use_gpu=False
                    )
                    print("âœ“ Stanzaè‹±æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ")
                except Exception as e2:
                    print(f"Stanzaè‹±æ–‡æ¨¡å‹åŠ è½½å¤±è´¥: {e2}")
                    print("ğŸ’¡ æç¤º: è¿è¡Œ 'python download_models.py' é¢„ä¸‹è½½æ¨¡å‹æ–‡ä»¶")

        # åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†ææ¨¡å‹
        self._init_advanced_sentiment_models()

        # åˆå§‹åŒ–SnowNLP
        if SNOWNLP_AVAILABLE:
            try:
                # SnowNLPä¸éœ€è¦é¢„åŠ è½½ï¼Œä½¿ç”¨æ—¶ç›´æ¥åˆ›å»ºå®ä¾‹
                self.nlp_models['snownlp'] = True  # æ ‡è®°å¯ç”¨
                print("âœ“ SnowNLPä¸­æ–‡æƒ…æ„Ÿåˆ†æå™¨å¯ç”¨")
            except Exception as e:
                print(f"SnowNLPåˆå§‹åŒ–å¤±è´¥: {e}")

        # æ·»åŠ åŸºç¡€NLPåŠŸèƒ½ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
        self._init_basic_nlp()

    def _init_advanced_sentiment_models(self):
        """åˆå§‹åŒ–æ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
        if not TRANSFORMERS_AVAILABLE:
            print("Transformersä¸å¯ç”¨ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹åˆå§‹åŒ–")
            return

        # é¢„è®­ç»ƒä¸­æ–‡æƒ…æ„Ÿåˆ†ææ¨¡å‹é…ç½®ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        sentiment_models = {
            'uer_roberta_dianping': {
                'model_name': 'uer/roberta-base-finetuned-dianping-chinese',
                'description': 'UERä¸­æ–‡RoBERTaæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆå¤§ä¼—ç‚¹è¯„æ•°æ®è®­ç»ƒï¼‰',
                'priority': 1
            },
            'erlangshen_roberta_110m': {
                'model_name': 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment',
                'description': 'äºŒéƒç¥RoBERTa-110Mæƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰',
                'priority': 2
            },
            'erlangshen_roberta_330m': {
                'model_name': 'IDEA-CCNL/Erlangshen-Roberta-330M-Sentiment',
                'description': 'äºŒéƒç¥RoBERTa-330Mæƒ…æ„Ÿåˆ†ææ¨¡å‹',
                'priority': 3
            },
        }

        # å°è¯•åŠ è½½é¢„è®­ç»ƒæƒ…æ„Ÿåˆ†ææ¨¡å‹
        loaded_models = 0
        for model_key, config in sentiment_models.items():
            try:
                print(f"å°è¯•åŠ è½½ {config['description']}...")

                # åˆ›å»ºæƒ…æ„Ÿåˆ†æpipeline
                sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model=config['model_name'],
                    tokenizer=config['model_name'],
                    return_all_scores=True
                )

                self.nlp_models[model_key] = sentiment_pipeline
                print(f"âœ“ {config['description']} åŠ è½½æˆåŠŸ")
                loaded_models += 1

                # å¦‚æœæˆåŠŸåŠ è½½äº†ä¸€ä¸ªé«˜ä¼˜å…ˆçº§æ¨¡å‹ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ç»§ç»­åŠ è½½å…¶ä»–æ¨¡å‹
                if config['priority'] == 1:
                    print("å·²åŠ è½½é«˜ä¼˜å…ˆçº§æ¨¡å‹ï¼Œè·³è¿‡å…¶ä»–æ¨¡å‹ä»¥èŠ‚çœå†…å­˜")
                    break

            except Exception as e:
                print(f"âœ— {config['description']} åŠ è½½å¤±è´¥: {e}")
                continue

        # å¦‚æœæ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°è¯•åŠ è½½é€šç”¨ä¸­æ–‡æ¨¡å‹
        if loaded_models == 0:
            try:
                print("å°è¯•åŠ è½½é€šç”¨ä¸­æ–‡BERTæ¨¡å‹...")
                # ä½¿ç”¨é€šç”¨çš„ä¸­æ–‡BERTæ¨¡å‹
                tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
                model = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese')

                self.nlp_models['bert_base_chinese'] = {
                    'tokenizer': tokenizer,
                    'model': model
                }
                print("âœ“ é€šç”¨ä¸­æ–‡BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"é€šç”¨ä¸­æ–‡BERTæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

        if loaded_models > 0:
            print(f"âœ“ æˆåŠŸåŠ è½½ {loaded_models} ä¸ªæ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†ææ¨¡å‹")
        else:
            print("âš  æœªèƒ½åŠ è½½ä»»ä½•æ·±åº¦å­¦ä¹ æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")

    def _init_basic_nlp(self):
        """åˆå§‹åŒ–åŸºç¡€NLPåŠŸèƒ½"""
        # æ‰©å±•çš„æƒ…æ„Ÿè¯å…¸
        self.sentiment_dict = {
            'positive': [
                # ä¸­æ–‡ç§¯æè¯æ±‡ - åŸºç¡€æƒ…æ„Ÿ
                'å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'æ»¡æ„', 'èµ', 'å®Œç¾', 'æˆåŠŸ', 'èƒœåˆ©',
                'ç¾å¥½', 'å¹¸ç¦', 'å¿«ä¹', 'å…´å¥‹', 'æ¿€åŠ¨', 'æƒŠå–œ', 'æ¸©æš–', 'èˆ’é€‚', 'å®‰å…¨', 'æ”¾å¿ƒ',
                'ä¿¡ä»»', 'å¸Œæœ›', 'ä¹è§‚', 'ç§¯æ', 'æ­£é¢', 'æœ‰è¶£', 'ç²¾å½©', 'å‡ºè‰²', 'å“è¶Š', 'æ°å‡º',
                'ä¼˜è´¨', 'ä¼˜è‰¯', 'å…ˆè¿›', 'åˆ›æ–°', 'çªç ´', 'è¿›æ­¥', 'å‘å±•', 'ç¹è£', 'æ˜Œç››', 'å…´æ—º',
                # ä¸­æ–‡ç§¯æè¯æ±‡ - æ‰©å±•
                'å¾ˆæ£’', 'éå¸¸å¥½', 'å¤ªå¥½äº†', 'çœŸæ£’', 'å¾ˆèµ', 'ä¸é”™', 'æŒºå¥½', 'å¾ˆæ»¡æ„', 'å¾ˆå–œæ¬¢',
                'æ¨è', 'å¼ºçƒˆæ¨è', 'å€¼å¾—', 'è¶…å€¼', 'ç‰©è¶…æ‰€å€¼', 'æ€§ä»·æ¯”é«˜', 'è´¨é‡å¥½', 'æœåŠ¡å¥½',
                'æ€åº¦å¥½', 'ç¯å¢ƒå¥½', 'å‘³é“å¥½', 'æ•ˆæœå¥½', 'ä½“éªŒå¥½', 'æ„Ÿè§‰å¥½', 'å¿ƒæƒ…å¥½', 'é¡ºåˆ©',
                'æˆå°±æ„Ÿ', 'æœ‰æˆå°±æ„Ÿ', 'å¾ˆæœ‰ç”¨', 'æœ‰å¸®åŠ©', 'æœ‰æ•ˆ', 'ç®¡ç”¨', 'é è°±', 'ç»™åŠ›',
                # è‹±æ–‡ç§¯æè¯æ±‡
                'excellent', 'good', 'great', 'love', 'like', 'happy', 'amazing', 'wonderful',
                'fantastic', 'awesome', 'brilliant', 'perfect', 'outstanding', 'superb',
                'magnificent', 'marvelous', 'delightful', 'pleasant', 'enjoyable', 'satisfying'
            ],
            'negative': [
                # ä¸­æ–‡æ¶ˆæè¯æ±‡ - åŸºç¡€æƒ…æ„Ÿ
                'å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'æ¨', 'éš¾è¿‡', 'å¤±æœ›', 'æ„¤æ€’', 'åƒåœ¾', 'çƒ‚', 'ç—›è‹¦',
                'æ‚²ä¼¤', 'æ²®ä¸§', 'ç»æœ›', 'ææƒ§', 'å®³æ€•', 'æ‹…å¿ƒ', 'ç„¦è™‘', 'ç´§å¼ ', 'å‹åŠ›', 'å›°éš¾',
                'é—®é¢˜', 'é”™è¯¯', 'å¤±è´¥', 'æŒ«æŠ˜', 'å±é™©', 'å¨èƒ', 'æŸå¤±', 'ç ´å', 'æ±¡æŸ“', 'è…è´¥',
                'æ¬ºéª—', 'è™šå‡', 'ä¸è‰¯', 'æ¶åŠ£', 'ä½åŠ£', 'è½å', 'è¡°é€€', 'å±æœº', 'ç¾éš¾', 'æ‚²å‰§',
                # ä¸­æ–‡æ¶ˆæè¯æ±‡ - æ‰©å±•
                'å¾ˆå·®', 'å¤ªå·®äº†', 'å¾ˆç³Ÿç³•', 'å¤ªç³Ÿç³•äº†', 'å¾ˆå¤±æœ›', 'å¤ªå¤±æœ›äº†', 'ä¸æ»¡æ„', 'ä¸æ¨è',
                'å®Œå…¨ä¸æ¨è', 'ä¸å€¼å¾—', 'ä¸åˆ’ç®—', 'è´¨é‡å·®', 'æœåŠ¡å·®', 'æ€åº¦å·®', 'ç¯å¢ƒå·®', 'å‘³é“å·®',
                'æ•ˆæœå·®', 'ä½“éªŒå·®', 'æ„Ÿè§‰å·®', 'å¿ƒæƒ…å·®', 'ä¸é¡ºåˆ©', 'æœ‰é—®é¢˜', 'å‡ºé—®é¢˜', 'å‡ºbug',
                'éš¾ç”¨', 'å¤ªéš¾ç”¨äº†', 'ä¸å¥½ç”¨', 'ä¸ç®¡ç”¨', 'æ²¡ç”¨', 'æ— æ•ˆ', 'ä¸é è°±', 'ä¸ç»™åŠ›',
                'ç•Œé¢è®¾è®¡å¾ˆç³Ÿç³•', 'ç»å¸¸å‡ºç°bug', 'å¾ˆæ²®ä¸§', 'å¾ˆå¤±æœ›', 'ä¸å¥½', 'å¾ˆéš¾',
                # è‹±æ–‡æ¶ˆæè¯æ±‡
                'bad', 'terrible', 'awful', 'hate', 'sad', 'angry', 'disappointed', 'horrible',
                'disgusting', 'annoying', 'frustrating', 'depressing', 'disturbing', 'shocking',
                'devastating', 'tragic', 'disastrous', 'pathetic', 'miserable', 'dreadful'
            ],
            # æ·»åŠ å¦å®šè¯å’Œç¨‹åº¦å‰¯è¯
            'negation': ['ä¸', 'æ²¡', 'æ— ', 'é', 'æœª', 'åˆ«', 'å‹¿', 'è«', 'å¦', 'ä¸æ˜¯', 'æ²¡æœ‰', 'ä¸ä¼š', 'ä¸èƒ½', 'ä¸è¦', 'ä¸å¤ª', 'ä¸å¤Ÿ', 'ä¸æ€ä¹ˆ'],
            'intensifiers': {
                'strong': ['éå¸¸', 'ç‰¹åˆ«', 'æå…¶', 'ååˆ†', 'ç›¸å½“', 'è¶…çº§', 'å¤ª', 'å¾ˆ', 'æŒº', 'è›®', 'è¶…', 'å·¨', 'è¶…çº§', 'æåº¦'],
                'weak': ['æœ‰ç‚¹', 'ç¨å¾®', 'ç•¥å¾®', 'è¿˜ç®—', 'æ¯”è¾ƒ', 'ç›¸å¯¹', 'ç®—æ˜¯', 'ç¨', 'ç•¥']
            },
            # æ·»åŠ å¤åˆå¦å®šæ¨¡å¼
            'complex_negation': [
                'ä¸æ˜¯å¾ˆ', 'ä¸å¤ª', 'ä¸å¤Ÿ', 'ä¸æ€ä¹ˆ', 'æ²¡é‚£ä¹ˆ', 'ä¸ç®—', 'ç§°ä¸ä¸Š'
            ]
        }

        # æ”¹è¿›çš„å®ä½“æ¨¡å¼
        self.entity_patterns = {
            'PERSON': [
                r'(?<![a-zA-Z\u4e00-\u9fff])[A-Z][a-z]+ [A-Z][a-z]+(?![a-zA-Z\u4e00-\u9fff])',  # è‹±æ–‡äººåï¼ˆæ”¹è¿›è¾¹ç•Œï¼‰
                r'(?<![a-zA-Z\u4e00-\u9fff])[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå´å‘¨å¾å­™é©¬æœ±èƒ¡éƒ­ä½•é«˜æ—ç½—éƒ‘æ¢è°¢å®‹å”è®¸éŸ©å†¯é‚“æ›¹å½­æ›¾è‚–ç”°è‘£è¢æ½˜äºè’‹è”¡ä½™æœå¶ç¨‹è‹é­å•ä¸ä»»æ²ˆå§šå¢å§œå´”é’Ÿè°­é™†æ±ªèŒƒé‡‘çŸ³å»–è´¾å¤éŸ¦ä»˜æ–¹ç™½é‚¹å­Ÿç†Šç§¦é‚±æ±Ÿå°¹è–›é—«æ®µé›·ä¾¯é¾™å²é™¶é»è´ºé¡¾æ¯›éƒé¾šé‚µä¸‡é’±ä¸¥è¦ƒæ­¦æˆ´è«å­”å‘æ±¤][\u4e00-\u9fff]{1,3}(?![a-zA-Z\u4e00-\u9fff])',  # ä¸­æ–‡å§“åï¼ˆæ”¹è¿›è¾¹ç•Œï¼‰
                r'å²è’‚å¤«Â·[a-zA-Z\u4e00-\u9fff]+',  # ç‰¹æ®Šæ ¼å¼äººå
                r'[a-zA-Z\u4e00-\u9fff]+Â·[a-zA-Z\u4e00-\u9fff]+',  # ä¸­é—´æœ‰ç‚¹çš„äººå
            ],
            'ORG': [
                r'[\u4e00-\u9fff]{2,10}(?:å…¬å¸|å¤§å­¦|å­¦é™¢|åŒ»é™¢|é“¶è¡Œ|é›†å›¢|ä¼ä¸š|æœºæ„|ç»„ç»‡|ç ”ç©¶æ‰€|ç ”ç©¶é™¢|åŸºé‡‘ä¼š|åä¼š|è”ç›Ÿ)',  # ä¸­æ–‡æœºæ„ï¼ˆæ”¹è¿›ï¼‰
                r'[A-Z][a-zA-Z\s]{2,30}(?:Company|Corp|Inc|Ltd|University|College|Hospital|Bank|Institute|Foundation|Association)',  # è‹±æ–‡æœºæ„ï¼ˆæ”¹è¿›ï¼‰
                r'è‹¹æœå…¬å¸|å¾®è½¯å…¬å¸|è°·æ­Œå…¬å¸|è…¾è®¯å…¬å¸|é˜¿é‡Œå·´å·´|ç™¾åº¦å…¬å¸',  # çŸ¥åå…¬å¸
            ],
            'LOC': [
                r'(?:åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|æ­å·|å—äº¬|æ­¦æ±‰|æˆéƒ½|é‡åº†|å¤©æ´¥|è¥¿å®‰|æ²ˆé˜³|é•¿æ²™|å“ˆå°”æ»¨|æ˜†æ˜|å¤§è¿|é’å²›|å®æ³¢|å¦é—¨|è‹å·|æ— é”¡|ç¦å·|æµå—|å¤ªåŸ|é•¿æ˜¥|çŸ³å®¶åº„|å—æ˜Œ|è´µé˜³|å—å®|å…°å·|é“¶å·|è¥¿å®|ä¹Œé²æœ¨é½|å‘¼å’Œæµ©ç‰¹|æ‹‰è¨|æµ·å£|ä¸‰äºš)å¸‚?',  # ä¸­å›½åŸå¸‚ï¼ˆæ”¹è¿›ï¼‰
                r'(?:æ²³åŒ—|å±±è¥¿|è¾½å®|å‰æ—|é»‘é¾™æ±Ÿ|æ±Ÿè‹|æµ™æ±Ÿ|å®‰å¾½|ç¦å»º|æ±Ÿè¥¿|å±±ä¸œ|æ²³å—|æ¹–åŒ—|æ¹–å—|å¹¿ä¸œ|æµ·å—|å››å·|è´µå·|äº‘å—|é™•è¥¿|ç”˜è‚ƒ|é’æµ·|å°æ¹¾|å†…è’™å¤|å¹¿è¥¿|è¥¿è—|å®å¤|æ–°ç–†)(?:çœ|è‡ªæ²»åŒº)?',  # ä¸­å›½çœä»½ï¼ˆæ”¹è¿›ï¼‰
                r'(?:ç¾å›½|ä¸­å›½|æ—¥æœ¬|è‹±å›½|æ³•å›½|å¾·å›½|æ„å¤§åˆ©|åŠ æ‹¿å¤§|æ¾³å¤§åˆ©äºš|éŸ©å›½|å°åº¦|å·´è¥¿|ä¿„ç½—æ–¯)(?![a-zA-Z\u4e00-\u9fff])',  # å›½å®¶
                r'åŠ åˆ©ç¦å°¼äºšå·|çº½çº¦å·|å¾·å…‹è¨æ–¯å·',  # ç¾å›½å·
            ],
            'TIME': [
                r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',  # ä¸­æ–‡æ—¥æœŸ
                r'\d{4}å¹´\d{1,2}æœˆ',  # ä¸­æ–‡å¹´æœˆ
                r'\d{4}å¹´',  # å¹´ä»½
                r'(?:19|20)\d{2}å¹´ä»£',  # å¹´ä»£
                r'\d{1,2}ä¸–çºª',  # ä¸–çºª
            ]
        }

        print("âœ“ åŸºç¡€NLPåŠŸèƒ½åˆå§‹åŒ–å®Œæˆ")

    def _init_segmenters(self):
        """åˆå§‹åŒ–ä¸­æ–‡åˆ†è¯å™¨"""
        # åˆå§‹åŒ–jiebaåˆ†è¯å™¨
        if JIEBA_AVAILABLE:
            try:
                # è®¾ç½®jiebaä¸ºé™é»˜æ¨¡å¼
                jieba.setLogLevel(20)
                # é¢„åŠ è½½è¯å…¸
                jieba.initialize()
                self.segmenters['jieba'] = jieba
                print("âœ“ jiebaåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"jiebaåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")

        # åˆå§‹åŒ–pkusegåˆ†è¯å™¨
        if PKUSEG_AVAILABLE:
            try:
                # ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼Œæ”¯æŒå¤šç§é¢†åŸŸ
                self.segmenters['pkuseg_default'] = pkuseg.pkuseg()
                print("âœ“ pkusegé»˜è®¤åˆ†è¯å™¨åŠ è½½æˆåŠŸ")

                # å°è¯•åŠ è½½ä¸åŒé¢†åŸŸçš„æ¨¡å‹
                domain_models = {
                    'pkuseg_news': 'news',      # æ–°é—»é¢†åŸŸ
                    'pkuseg_web': 'web',        # ç½‘ç»œé¢†åŸŸ
                    'pkuseg_medicine': 'medicine',  # åŒ»è¯é¢†åŸŸ
                    'pkuseg_tourism': 'tourism'     # æ—…æ¸¸é¢†åŸŸ
                }

                for model_key, domain in domain_models.items():
                    try:
                        self.segmenters[model_key] = pkuseg.pkuseg(model_name=domain)
                        print(f"âœ“ pkuseg {domain}é¢†åŸŸåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
                    except Exception as e:
                        print(f"pkuseg {domain}é¢†åŸŸåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")

            except Exception as e:
                print(f"pkusegåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")

        # åˆå§‹åŒ–thulacåˆ†è¯å™¨
        if THULAC_AVAILABLE:
            try:
                # ä½¿ç”¨é»˜è®¤æ¨¡å‹
                self.segmenters['thulac'] = thulac.thulac()
                print("âœ“ thulacåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"thulacåˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")

        print("âœ“ åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ")

    def _init_textteaser(self):
        """åˆå§‹åŒ–TextTeaseræ‘˜è¦å™¨ï¼ˆè½»é‡çº§å®ç°ï¼‰"""
        if TEXTTEASER_AVAILABLE:
            # æˆ‘ä»¬ä½¿ç”¨è‡ªå·±çš„è½»é‡çº§å®ç°ï¼Œä¸éœ€è¦å¤–éƒ¨åº“
            self.textteaser = True  # æ ‡è®°ä¸ºå¯ç”¨
            print("âœ“ è½»é‡çº§TextTeaserç®—æ³•åŠ è½½æˆåŠŸ")
        else:
            self.textteaser = None

    def _init_qwen3(self):
        """åˆå§‹åŒ–Qwen3å¤§æ¨¡å‹å®¢æˆ·ç«¯"""
        if QWEN3_AVAILABLE:
            try:
                # è®¾ç½®APIç«¯ç‚¹
                self.qwen3_api_url = 'http://localhost:6006/api/chat'
                self.qwen3_model = 'qwen3:8b'  # æ ¹æ®æ‚¨æä¾›çš„æ¨¡å‹ä¿¡æ¯

                # æµ‹è¯•è¿æ¥
                test_data = {
                    "model": self.qwen3_model,
                    "messages": [
                        {
                            "role": "user",
                            "content": "æµ‹è¯•è¿æ¥"
                        }
                    ],
                    "stream": False
                }

                headers = {'Content-type': 'application/json'}
                response = requests.post(self.qwen3_api_url,
                                       data=json.dumps(test_data),
                                       headers=headers,
                                       timeout=10)

                if response.status_code == 200:
                    self.qwen3_client = True  # æ ‡è®°ä¸ºå¯ç”¨
                    print(f"âœ“ Qwen3æ¨¡å‹è¿æ¥æˆåŠŸ: {self.qwen3_model}")
                else:
                    print(f"âš  Qwen3æ¨¡å‹è¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    self.qwen3_client = None

            except Exception as e:
                print(f"Qwen3æ¨¡å‹è¿æ¥å¤±è´¥: {e}")
                self.qwen3_client = None
        else:
            self.qwen3_client = None

    def segment_text(self, text: Optional[str] = None,
                    method: str = 'auto',
                    mode: str = 'accurate',
                    with_pos: bool = False) -> List[Dict]:
        """
        ä¸­æ–‡åˆ†è¯åŠŸèƒ½

        Args:
            text: è¦åˆ†è¯çš„æ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ–‡æœ¬
            method: åˆ†è¯æ–¹æ³• ('auto', 'jieba', 'pkuseg', 'thulac', 'basic')
            mode: åˆ†è¯æ¨¡å¼ ('accurate', 'full', 'search') - ä»…jiebaæ”¯æŒ
            with_pos: æ˜¯å¦åŒ…å«è¯æ€§æ ‡æ³¨

        Returns:
            [{'word': 'è¯', 'pos': 'è¯æ€§'}, ...] æˆ– [{'word': 'è¯'}, ...]
        """
        if text is None:
            text = self.text

        if not text.strip():
            return []

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³åˆ†è¯å™¨
        if method == 'auto':
            if 'pkuseg_default' in self.segmenters:
                method = 'pkuseg_default'  # pkusegå‡†ç¡®åº¦è¾ƒé«˜
            elif 'jieba' in self.segmenters:
                method = 'jieba'   # jiebaé€Ÿåº¦è¾ƒå¿«
            elif 'thulac' in self.segmenters:
                method = 'thulac'  # thulacä¹Ÿä¸é”™
            else:
                method = 'basic'   # é™çº§åˆ°åŸºç¡€æ–¹æ³•

        # ä½¿ç”¨æŒ‡å®šçš„åˆ†è¯å™¨
        if method == 'jieba' and 'jieba' in self.segmenters:
            return self._jieba_segment(text, mode, with_pos)
        elif method.startswith('pkuseg') and method in self.segmenters:
            return self._pkuseg_segment(text, with_pos, method)
        elif method == 'thulac' and 'thulac' in self.segmenters:
            return self._thulac_segment(text, with_pos)
        else:
            return self._basic_segment(text, with_pos)

    def _jieba_segment(self, text: str, mode: str = 'accurate', with_pos: bool = False) -> List[Dict]:
        """ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯"""
        try:
            if with_pos:
                # å¸¦è¯æ€§æ ‡æ³¨çš„åˆ†è¯
                words = pseg.cut(text)
                return [{'word': word, 'pos': pos} for word, pos in words if word.strip()]
            else:
                # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†è¯æ–¹æ³•
                if mode == 'accurate':
                    words = jieba.cut(text, cut_all=False)
                elif mode == 'full':
                    words = jieba.cut(text, cut_all=True)
                elif mode == 'search':
                    words = jieba.cut_for_search(text)
                else:
                    words = jieba.cut(text, cut_all=False)

                return [{'word': word} for word in words if word.strip()]
        except Exception as e:
            print(f"jiebaåˆ†è¯å¤±è´¥: {e}")
            return self._basic_segment(text, with_pos)

    def _pkuseg_segment(self, text: str, with_pos: bool = False, model_key: str = 'pkuseg_default') -> List[Dict]:
        """ä½¿ç”¨pkusegè¿›è¡Œåˆ†è¯"""
        try:
            segmenter = self.segmenters.get(model_key)
            if not segmenter:
                # å¦‚æœæŒ‡å®šæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
                segmenter = self.segmenters.get('pkuseg_default')
                if not segmenter:
                    return self._basic_segment(text, with_pos)

            if with_pos:
                # ä½¿ç”¨å¸¦è¯æ€§æ ‡æ³¨çš„pkuseg
                try:
                    # åˆ›å»ºå¸¦è¯æ€§æ ‡æ³¨çš„åˆ†è¯å™¨
                    pos_segmenter = pkuseg.pkuseg(postag=True)
                    words_pos = pos_segmenter.cut(text)
                    return [{'word': word, 'pos': pos} for word, pos in words_pos if word.strip()]
                except:
                    # å¦‚æœè¯æ€§æ ‡æ³¨å¤±è´¥ï¼Œé™çº§åˆ°æ™®é€šåˆ†è¯
                    words = segmenter.cut(text)
                    return [{'word': word, 'pos': 'UNK'} for word in words if word.strip()]
            else:
                words = segmenter.cut(text)
                return [{'word': word} for word in words if word.strip()]
        except Exception as e:
            print(f"pkusegåˆ†è¯å¤±è´¥: {e}")
            return self._basic_segment(text, with_pos)

    def _thulac_segment(self, text: str, with_pos: bool = False) -> List[Dict]:
        """ä½¿ç”¨thulacè¿›è¡Œåˆ†è¯"""
        try:
            words_pos = self.segmenters['thulac'].cut(text)
            if with_pos:
                return [{'word': word, 'pos': pos} for word, pos in words_pos if word.strip()]
            else:
                return [{'word': word} for word, pos in words_pos if word.strip()]
        except Exception as e:
            print(f"thulacåˆ†è¯å¤±è´¥: {e}")
            return self._basic_segment(text, with_pos)

    def _basic_segment(self, text: str, with_pos: bool = False) -> List[Dict]:
        """åŸºç¡€åˆ†è¯æ–¹æ³•ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºç™½å­—ç¬¦åˆ†å‰²ï¼‰"""
        # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·
        chinese_punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€""â€¦â€¦'
        # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
        english_punctuation = string.punctuation
        all_punctuation = chinese_punctuation + english_punctuation

        # å°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼
        for punct in all_punctuation:
            text = text.replace(punct, ' ')

        # å°†æ¢è¡Œç¬¦ä¹Ÿæ›¿æ¢ä¸ºç©ºæ ¼
        text = text.replace('\n', ' ').replace('\r', ' ')

        # åˆ†è¯ï¼ˆæŒ‰ç©ºç™½å­—ç¬¦åˆ†å‰²ï¼Œå¹¶è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ï¼‰
        words = [word.strip() for word in text.split() if word.strip()]

        if with_pos:
            return [{'word': word, 'pos': 'UNK'} for word in words]
        else:
            return [{'word': word} for word in words]

    def extract_entities(self, text: Optional[str] = None, method: str = 'hybrid',
                        deduplicate: bool = True) -> Dict[str, List[Dict]]:
        """
        å®ä½“è¯†åˆ«åŠŸèƒ½

        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ–‡æœ¬
            method: è¯†åˆ«æ–¹æ³• ('spacy', 'regex', 'hybrid')
            deduplicate: æ˜¯å¦å¯¹å®ä½“è¿›è¡Œå»é‡å’Œç»Ÿè®¡

        Returns:
            {
                'entities': [{'text': 'å®ä½“æ–‡æœ¬', 'label': 'å®ä½“ç±»å‹', 'count': å‡ºç°æ¬¡æ•°, 'positions': [ä½ç½®åˆ—è¡¨]}],
                'available': æ˜¯å¦å¯ç”¨,
                'model_used': ä½¿ç”¨çš„æ¨¡å‹,
                'deduplicated': æ˜¯å¦å·²å»é‡
            }
        """
        if text is None:
            text = self.text

        if not text.strip():
            return {'entities': [], 'available': False, 'model_used': None, 'deduplicated': False}

        # æ‰§è¡Œå®ä½“è¯†åˆ«
        if method == 'regex':
            result = self._basic_entity_recognition(text)
        elif method == 'spacy':
            result = self._spacy_entity_recognition(text)
        else:  # method == 'hybrid'
            result = self._hybrid_entity_recognition(text)

        # å¦‚æœéœ€è¦å»é‡
        if deduplicate and result['available'] and result['entities']:
            result['entities'] = self._deduplicate_entities(result['entities'])
            result['deduplicated'] = True
        else:
            result['deduplicated'] = False

        return result

    def _spacy_entity_recognition(self, text: str) -> Dict[str, List[Dict]]:
        """ä½¿ç”¨spaCyè¿›è¡Œå®ä½“è¯†åˆ«"""
        # ä¼˜å…ˆä½¿ç”¨ä¸­æ–‡spaCyæ¨¡å‹
        if 'spacy_zh' in self.nlp_models:
            doc = self.nlp_models['spacy_zh'](text)
            entities = []
            for ent in doc.ents:
                entities.append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char,
                    'description': spacy.explain(ent.label_) or ent.label_,
                    'confidence': 1.0  # spaCyä¸æä¾›ç½®ä¿¡åº¦ï¼Œè®¾ä¸º1.0
                })
            return {
                'entities': entities,
                'available': True,
                'model_used': 'spacy_zh'
            }

        # ä½¿ç”¨è‹±æ–‡spaCyæ¨¡å‹
        elif 'spacy_en' in self.nlp_models:
            doc = self.nlp_models['spacy_en'](text)
            entities = []
            for ent in doc.ents:
                entities.append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char,
                    'end': ent.end_char,
                    'description': spacy.explain(ent.label_) or ent.label_,
                    'confidence': 1.0
                })
            return {
                'entities': entities,
                'available': True,
                'model_used': 'spacy_en'
            }

        # é™çº§åˆ°åŸºç¡€å®ä½“è¯†åˆ«
        return self._basic_entity_recognition(text)

    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:
        """
        å¯¹å®ä½“è¿›è¡Œå»é‡å’Œç»Ÿè®¡

        Args:
            entities: åŸå§‹å®ä½“åˆ—è¡¨

        Returns:
            å»é‡åçš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«ç»Ÿè®¡ä¿¡æ¯
        """
        entity_stats = {}

        for entity in entities:
            # ä½¿ç”¨å®ä½“æ–‡æœ¬å’Œæ ‡ç­¾ä½œä¸ºå”¯ä¸€æ ‡è¯†
            key = (entity['text'].strip().lower(), entity['label'])

            if key not in entity_stats:
                entity_stats[key] = {
                    'text': entity['text'].strip(),
                    'label': entity['label'],
                    'description': entity.get('description', entity['label']),
                    'count': 0,
                    'positions': [],
                    'sources': set(),
                    'confidence': entity.get('confidence', 1.0)
                }

            entity_stats[key]['count'] += 1
            entity_stats[key]['positions'].append({
                'start': entity['start'],
                'end': entity['end']
            })

            if 'source' in entity:
                entity_stats[key]['sources'].add(entity['source'])

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        deduplicated = []
        for (text, label), stats in entity_stats.items():
            deduplicated.append({
                'text': stats['text'],
                'label': label,
                'description': stats['description'],
                'count': stats['count'],
                'positions': stats['positions'],
                'sources': list(stats['sources']) if stats['sources'] else [],
                'confidence': stats['confidence']
            })

        # æŒ‰å‡ºç°æ¬¡æ•°é™åºæ’åˆ—
        deduplicated.sort(key=lambda x: x['count'], reverse=True)

        return deduplicated

    def _hybrid_entity_recognition(self, text: str) -> Dict[str, List[Dict]]:
        """æ··åˆå®ä½“è¯†åˆ«ï¼šç»“åˆspaCyå’Œæ­£åˆ™è¡¨è¾¾å¼"""
        # è·å–spaCyè¯†åˆ«ç»“æœ
        spacy_result = self._spacy_entity_recognition(text)

        # è·å–æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«ç»“æœ
        regex_result = self._basic_entity_recognition(text)

        if not spacy_result['available']:
            return regex_result

        # åˆå¹¶å’Œè¿‡æ»¤ç»“æœ
        merged_entities = []

        # 1. æ·»åŠ é«˜è´¨é‡çš„spaCyç»“æœï¼ˆè¿‡æ»¤æ‰ä¸€äº›ä¸å¯é çš„ç±»å‹ï¼‰
        reliable_spacy_labels = {'PERSON', 'ORG', 'GPE', 'DATE', 'TIME', 'MONEY', 'PERCENT'}
        for entity in spacy_result['entities']:
            if entity['label'] in reliable_spacy_labels and len(entity['text'].strip()) > 1:
                # è¿‡æ»¤æ‰è¿‡çŸ­æˆ–åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„å®ä½“
                if not re.match(r'^[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\s]+$', entity['text']):
                    entity['source'] = 'spacy'
                    merged_entities.append(entity)

        # 2. æ·»åŠ æ­£åˆ™è¡¨è¾¾å¼çš„é«˜ç²¾åº¦ç»“æœï¼ˆé¿å…é‡å¤ï¼‰
        for regex_entity in regex_result['entities']:
            # æ£€æŸ¥æ˜¯å¦ä¸spaCyç»“æœé‡å 
            is_duplicate = False
            for spacy_entity in merged_entities:
                if self._entities_overlap(regex_entity, spacy_entity):
                    is_duplicate = True
                    break

            if not is_duplicate:
                regex_entity['source'] = 'regex'
                merged_entities.append(regex_entity)

        # 3. æŒ‰ä½ç½®æ’åº
        merged_entities.sort(key=lambda x: x['start'])

        return {
            'entities': merged_entities,
            'available': True,
            'model_used': 'hybrid_spacy_regex'
        }

    def _entities_overlap(self, entity1: Dict, entity2: Dict) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå®ä½“æ˜¯å¦é‡å """
        return not (entity1['end'] <= entity2['start'] or entity2['end'] <= entity1['start'])

    def _basic_entity_recognition(self, text: str) -> Dict[str, List[Dict]]:
        """åŸºç¡€å®ä½“è¯†åˆ«ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼‰"""
        entities = []

        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text)
                for match in matches:
                    entities.append({
                        'text': match.group(),
                        'label': entity_type,
                        'start': match.start(),
                        'end': match.end(),
                        'description': self._get_entity_description(entity_type),
                        'confidence': 0.9  # æ­£åˆ™è¡¨è¾¾å¼ç½®ä¿¡åº¦
                    })

        return {
            'entities': entities,
            'available': True,
            'model_used': 'basic_regex'
        }

    def _get_entity_description(self, entity_type: str) -> str:
        """è·å–å®ä½“ç±»å‹æè¿°"""
        descriptions = {
            'PERSON': 'äººå',
            'ORG': 'æœºæ„ç»„ç»‡',
            'LOC': 'åœ°ç‚¹ä½ç½®',
            'TIME': 'æ—¶é—´æ—¥æœŸ'
        }
        return descriptions.get(entity_type, entity_type)

    def analyze_sentiment(self, text: Optional[str] = None) -> Dict:
        """
        æƒ…æ„Ÿåˆ†æåŠŸèƒ½

        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ–‡æœ¬

        Returns:
            {
                'sentiment': æƒ…æ„Ÿå€¾å‘,
                'scores': è¯¦ç»†åˆ†æ•°,
                'available': æ˜¯å¦å¯ç”¨,
                'methods_used': ä½¿ç”¨çš„æ–¹æ³•åˆ—è¡¨
            }
        """
        if text is None:
            text = self.text

        if not text.strip():
            return {'sentiment': 'neutral', 'scores': {}, 'available': False, 'methods_used': []}

        results = {
            'sentiment': 'neutral',
            'scores': {},
            'available': False,
            'methods_used': [],
            'confidence': 0.0,
            'model_details': {}
        }

        # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œç”¨äºèåˆ
        all_predictions = []

        # 1. ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        dl_result = self._analyze_with_deep_learning(text)
        if dl_result['available']:
            results['scores'].update(dl_result['scores'])
            results['methods_used'].extend(dl_result['methods_used'])
            results['model_details'].update(dl_result['model_details'])
            results['available'] = True
            all_predictions.append({
                'sentiment': dl_result['sentiment'],
                'confidence': dl_result['confidence'],
                'weight': 0.35  # æ·±åº¦å­¦ä¹ æ¨¡å‹æƒé‡
            })

        # 2. ä½¿ç”¨SnowNLPè¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æï¼ˆæƒé‡æœ€é«˜ï¼Œå› ä¸ºä¸“é—¨é’ˆå¯¹ä¸­æ–‡ï¼‰
        snow_result = self._analyze_with_snownlp(text)
        if snow_result['available']:
            results['scores'].update(snow_result['scores'])
            results['methods_used'].extend(snow_result['methods_used'])
            results['available'] = True
            all_predictions.append({
                'sentiment': snow_result['sentiment'],
                'confidence': snow_result['confidence'],
                'weight': 0.35  # SnowNLPæƒé‡ï¼Œä¸“é—¨é’ˆå¯¹ä¸­æ–‡
            })

        # 3. ä½¿ç”¨VADERè¿›è¡Œæƒ…æ„Ÿåˆ†æï¼ˆå¯¹ä¸­æ–‡æ•ˆæœä¸å¥½ï¼Œé™ä½æƒé‡ï¼‰
        if 'vader' in self.nlp_models:
            vader_scores = self.nlp_models['vader'].polarity_scores(text)
            results['scores']['vader'] = vader_scores
            results['methods_used'].append('vader')
            results['available'] = True

            # æ ¹æ®compoundåˆ†æ•°ç¡®å®šæƒ…æ„Ÿå€¾å‘
            compound = vader_scores['compound']
            if compound >= 0.05:
                vader_sentiment = 'positive'
                vader_confidence = min(abs(compound), 1.0)
            elif compound <= -0.05:
                vader_sentiment = 'negative'
                vader_confidence = min(abs(compound), 1.0)
            else:
                vader_sentiment = 'neutral'
                vader_confidence = 1.0 - abs(compound)

            # åªæœ‰å½“VADERæœ‰æ˜ç¡®åˆ¤æ–­æ—¶æ‰åŠ å…¥èåˆï¼ˆé¿å…ä¸­æ€§ç»“æœå¹²æ‰°ï¼‰
            if abs(compound) > 0.1:
                all_predictions.append({
                    'sentiment': vader_sentiment,
                    'confidence': vader_confidence,
                    'weight': 0.10  # VADERæƒé‡è¾ƒä½ï¼Œå¯¹ä¸­æ–‡æ”¯æŒæœ‰é™
                })

        # 4. ä½¿ç”¨TextBlobè¿›è¡Œæƒ…æ„Ÿåˆ†æ
        if TEXTBLOB_AVAILABLE:
            try:
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                subjectivity = blob.sentiment.subjectivity

                results['scores']['textblob'] = {
                    'polarity': polarity,
                    'subjectivity': subjectivity
                }
                results['methods_used'].append('textblob')
                results['available'] = True

                # ç¡®å®šTextBlobçš„æƒ…æ„Ÿå€¾å‘
                if polarity > 0.1:
                    textblob_sentiment = 'positive'
                    textblob_confidence = min(abs(polarity), 1.0)
                elif polarity < -0.1:
                    textblob_sentiment = 'negative'
                    textblob_confidence = min(abs(polarity), 1.0)
                else:
                    textblob_sentiment = 'neutral'
                    textblob_confidence = 1.0 - abs(polarity)

                # åªæœ‰å½“TextBlobæœ‰æ˜ç¡®åˆ¤æ–­æ—¶æ‰åŠ å…¥èåˆ
                if abs(polarity) > 0.1:
                    all_predictions.append({
                        'sentiment': textblob_sentiment,
                        'confidence': textblob_confidence,
                        'weight': 0.05  # TextBlobæƒé‡å¾ˆä½ï¼Œä¸»è¦é’ˆå¯¹è‹±æ–‡
                    })
            except Exception:
                pass

        # 5. ä½¿ç”¨å¢å¼ºçš„åŸºç¡€æƒ…æ„Ÿåˆ†æï¼ˆæ€»æ˜¯è¿è¡Œï¼Œä½œä¸ºè¡¥å……ï¼‰
        basic_result = self._basic_sentiment_analysis(text)
        if basic_result['available']:
            results['scores'].update(basic_result['scores'])
            if not results['available']:  # å¦‚æœæ²¡æœ‰å…¶ä»–æ–¹æ³•å¯ç”¨
                results['methods_used'].extend(basic_result['methods_used'])
                results['available'] = True
                all_predictions.append({
                    'sentiment': basic_result['sentiment'],
                    'confidence': basic_result['confidence'],
                    'weight': 1.0  # å¦‚æœåªæœ‰åŸºç¡€æ–¹æ³•ï¼Œæƒé‡ä¸º1
                })
            else:  # ä½œä¸ºè¡¥å……æ–¹æ³•
                results['methods_used'].extend(basic_result['methods_used'])
                # åªæœ‰å½“åŸºç¡€æ–¹æ³•æœ‰æ˜ç¡®åˆ¤æ–­æ—¶æ‰åŠ å…¥èåˆ
                if basic_result['confidence'] > 0.6:
                    all_predictions.append({
                        'sentiment': basic_result['sentiment'],
                        'confidence': basic_result['confidence'],
                        'weight': 0.15  # åŸºç¡€æ–¹æ³•ä½œä¸ºè¡¥å……
                    })

        # 6. èåˆæ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        if all_predictions:
            final_result = self._ensemble_predictions(all_predictions)
            results['sentiment'] = final_result['sentiment']
            results['confidence'] = final_result['confidence']
            results['ensemble_details'] = final_result['details']

        return results

    def _analyze_with_deep_learning(self, text: str) -> Dict:
        """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ"""
        result = {
            'sentiment': 'neutral',
            'confidence': 0.0,
            'scores': {},
            'available': False,
            'methods_used': [],
            'model_details': {}
        }

        # æ£€æŸ¥å¯ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
        dl_models = [
            'uer_roberta_dianping', 'erlangshen_roberta_110m', 'erlangshen_roberta_330m',
            'chinese_roberta_wwm_ext', 'chinese_bert_wwm', 'bert_base_chinese'
        ]

        for model_key in dl_models:
            if model_key in self.nlp_models:
                try:
                    if model_key in ['bert_base_chinese', 'chinese_bert_wwm', 'chinese_roberta_wwm_ext']:
                        # å¤„ç†éœ€è¦å¾®è°ƒçš„é€šç”¨æ¨¡å‹ï¼ˆæš‚æ—¶è·³è¿‡ï¼‰
                        print(f"è·³è¿‡æœªå¾®è°ƒçš„æ¨¡å‹: {model_key}")
                        continue
                    else:
                        # å¤„ç†é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
                        dl_result = self._predict_with_pipeline(text, model_key)

                    if dl_result['available']:
                        result.update(dl_result)
                        break

                except Exception as e:
                    print(f"æ·±åº¦å­¦ä¹ æ¨¡å‹ {model_key} é¢„æµ‹å¤±è´¥: {e}")
                    continue

        return result

    def _predict_with_pipeline(self, text: str, model_key: str) -> Dict:
        """ä½¿ç”¨pipelineæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        try:
            pipeline_model = self.nlp_models[model_key]

            # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…å‡ºæ¨¡å‹é™åˆ¶
            max_length = 512
            if len(text) > max_length:
                text = text[:max_length]

            predictions = pipeline_model(text)

            # è§£æé¢„æµ‹ç»“æœ
            if predictions and len(predictions) > 0:
                # å¤„ç†ä¸åŒæ¨¡å‹çš„è¾“å‡ºæ ¼å¼
                if isinstance(predictions[0], list):
                    # å¦‚æœæ˜¯åµŒå¥—åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                    predictions = predictions[0]

                # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„é¢„æµ‹
                best_pred = max(predictions, key=lambda x: x['score'])

                # æ ‡å‡†åŒ–æ ‡ç­¾ï¼ˆæ”¯æŒæ›´å¤šæ ¼å¼ï¼‰
                label = str(best_pred['label']).lower().strip()
                score = float(best_pred['score'])

                # æ˜ å°„æ ‡ç­¾åˆ°æ ‡å‡†æ ¼å¼ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ ‡ç­¾ï¼‰
                sentiment = 'neutral'  # é»˜è®¤å€¼

                if any(pos_word in label for pos_word in ['pos', 'positive', '1', 'ç§¯æ', 'æ­£é¢', 'good']):
                    sentiment = 'positive'
                elif any(neg_word in label for neg_word in ['neg', 'negative', '0', 'æ¶ˆæ', 'è´Ÿé¢', 'bad']):
                    sentiment = 'negative'
                elif any(neu_word in label for neu_word in ['neu', 'neutral', 'ä¸­æ€§', 'normal']):
                    sentiment = 'neutral'
                else:
                    # å¦‚æœæ ‡ç­¾æ— æ³•è¯†åˆ«ï¼Œæ ¹æ®åˆ†æ•°åˆ¤æ–­
                    if score > 0.6:
                        sentiment = 'positive'
                    elif score < 0.4:
                        sentiment = 'negative'
                    else:
                        sentiment = 'neutral'

                # è®¡ç®—ç½®ä¿¡åº¦
                confidence = score if sentiment != 'neutral' else max(score, 1.0 - score)

                return {
                    'sentiment': sentiment,
                    'confidence': confidence,
                    'scores': {model_key: predictions},
                    'available': True,
                    'methods_used': [model_key],
                    'model_details': {model_key: {
                        'best_prediction': best_pred,
                        'all_predictions': predictions,
                        'model_type': 'transformer_pipeline'
                    }}
                }

        except Exception as e:
            print(f"Pipelineé¢„æµ‹å¤±è´¥: {e}")

        return {'available': False}

    def _predict_with_bert(self, text: str, model_key: str) -> Dict:
        """ä½¿ç”¨é€šç”¨BERTæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼ˆéœ€è¦è‡ªå®šä¹‰åˆ†ç±»é€»è¾‘ï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°è‡ªå®šä¹‰çš„BERTåˆ†ç±»é€»è¾‘
            # ç”±äºé€šç”¨BERTæ¨¡å‹æ²¡æœ‰é¢„è®­ç»ƒçš„æƒ…æ„Ÿåˆ†æå¤´ï¼Œè¿™é‡Œè¿”å›ä¸å¯ç”¨
            return {'available': False}
        except Exception as e:
            print(f"BERTé¢„æµ‹å¤±è´¥: {e}")
            return {'available': False}

    def _analyze_with_snownlp(self, text: str) -> Dict:
        """ä½¿ç”¨SnowNLPè¿›è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æ"""
        result = {
            'sentiment': 'neutral',
            'confidence': 0.0,
            'scores': {},
            'available': False,
            'methods_used': []
        }

        if not SNOWNLP_AVAILABLE or 'snownlp' not in self.nlp_models:
            return result

        try:
            snow = SnowNLP(text)
            sentiment_score = snow.sentiments  # è¿”å›0-1ä¹‹é—´çš„å€¼ï¼Œ>0.5ä¸ºç§¯æ

            # ç¡®å®šæƒ…æ„Ÿå€¾å‘ï¼ˆè°ƒæ•´é˜ˆå€¼ï¼Œè®©åˆ¤æ–­æ›´æ•æ„Ÿï¼‰
            if sentiment_score > 0.55:
                sentiment = 'positive'
                confidence = sentiment_score
            elif sentiment_score < 0.45:
                sentiment = 'negative'
                confidence = 1.0 - sentiment_score
            else:
                sentiment = 'neutral'
                confidence = 1.0 - abs(sentiment_score - 0.5) * 2

            result.update({
                'sentiment': sentiment,
                'confidence': confidence,
                'scores': {'snownlp': {'sentiment_score': sentiment_score}},
                'available': True,
                'methods_used': ['snownlp']
            })

        except Exception as e:
            print(f"SnowNLPåˆ†æå¤±è´¥: {e}")

        return result

    def _ensemble_predictions(self, predictions: List[Dict]) -> Dict:
        """èåˆå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ"""
        if not predictions:
            return {'sentiment': 'neutral', 'confidence': 0.0, 'details': {}}

        # è®¡ç®—åŠ æƒæŠ•ç¥¨
        sentiment_scores = {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
        total_weight = 0.0

        for pred in predictions:
            weight = pred['weight'] * pred['confidence']
            sentiment_scores[pred['sentiment']] += weight
            total_weight += weight

        # å½’ä¸€åŒ–åˆ†æ•°
        if total_weight > 0:
            for sentiment in sentiment_scores:
                sentiment_scores[sentiment] /= total_weight

        # é€‰æ‹©æœ€é«˜åˆ†æ•°çš„æƒ…æ„Ÿ
        final_sentiment = max(sentiment_scores, key=sentiment_scores.get)
        final_confidence = sentiment_scores[final_sentiment]

        return {
            'sentiment': final_sentiment,
            'confidence': final_confidence,
            'details': {
                'sentiment_scores': sentiment_scores,
                'total_predictions': len(predictions),
                'total_weight': total_weight
            }
        }

    def _basic_sentiment_analysis(self, text: str) -> Dict:
        """å¢å¼ºçš„åŸºç¡€æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºè¯å…¸ï¼Œæ”¯æŒå¦å®šè¯å’Œç¨‹åº¦å‰¯è¯ï¼‰"""
        # é¢„å¤„ç†æ–‡æœ¬
        text_clean = text.strip()

        # ä½¿ç”¨jiebaåˆ†è¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if JIEBA_AVAILABLE:
            try:
                words = list(jieba.cut(text_clean))
            except:
                words = text_clean.split()
        else:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
            import re
            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+|\d+', text_clean)

        positive_score = 0.0
        negative_score = 0.0
        sentiment_details = []

        # åˆ†ææ¯ä¸ªè¯åŠå…¶ä¸Šä¸‹æ–‡
        for i, word in enumerate(words):
            word_lower = word.lower()

            # æ£€æŸ¥æ˜¯å¦ä¸ºæƒ…æ„Ÿè¯
            is_positive = any(pos_word in word_lower for pos_word in self.sentiment_dict['positive'])
            is_negative = any(neg_word in word_lower for neg_word in self.sentiment_dict['negative'])

            if is_positive or is_negative:
                # åŸºç¡€åˆ†æ•°
                base_score = 1.0

                # æ£€æŸ¥ç¨‹åº¦å‰¯è¯
                intensifier_multiplier = 1.0
                for j in range(max(0, i-2), i):  # æ£€æŸ¥å‰ä¸¤ä¸ªè¯
                    prev_word = words[j]
                    if any(strong in prev_word for strong in self.sentiment_dict['intensifiers']['strong']):
                        intensifier_multiplier = 1.5
                        break
                    elif any(weak in prev_word for weak in self.sentiment_dict['intensifiers']['weak']):
                        intensifier_multiplier = 0.7
                        break

                # æ£€æŸ¥å¦å®šè¯ï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼‰
                is_negated = False

                # é¦–å…ˆæ£€æŸ¥å¤åˆå¦å®šæ¨¡å¼
                text_before_word = ''.join(words[max(0, i-3):i])
                for complex_neg in self.sentiment_dict['complex_negation']:
                    if complex_neg in text_before_word:
                        is_negated = True
                        break

                # å¦‚æœæ²¡æœ‰å¤åˆå¦å®šï¼Œæ£€æŸ¥ç®€å•å¦å®šè¯
                if not is_negated:
                    for j in range(max(0, i-3), i):  # æ£€æŸ¥å‰ä¸‰ä¸ªè¯
                        prev_word = words[j]
                        if any(neg in prev_word for neg in self.sentiment_dict['negation']):
                            is_negated = True
                            break

                # è®¡ç®—æœ€ç»ˆåˆ†æ•°
                final_score = base_score * intensifier_multiplier

                if is_positive:
                    if is_negated:
                        negative_score += final_score
                        sentiment_details.append(f"å¦å®šçš„ç§¯æè¯: {word}")
                    else:
                        positive_score += final_score
                        sentiment_details.append(f"ç§¯æè¯: {word}")
                elif is_negative:
                    if is_negated:
                        positive_score += final_score
                        sentiment_details.append(f"å¦å®šçš„æ¶ˆæè¯: {word}")
                    else:
                        negative_score += final_score
                        sentiment_details.append(f"æ¶ˆæè¯: {word}")

        # è®¡ç®—æƒ…æ„Ÿå€¾å‘
        total_score = positive_score + negative_score
        if total_score == 0:
            sentiment = 'neutral'
            polarity = 0.0
            confidence = 0.5
        else:
            polarity = (positive_score - negative_score) / total_score
            if polarity > 0.2:
                sentiment = 'positive'
                confidence = min(polarity + 0.5, 1.0)
            elif polarity < -0.2:
                sentiment = 'negative'
                confidence = min(abs(polarity) + 0.5, 1.0)
            else:
                sentiment = 'neutral'
                confidence = 1.0 - abs(polarity)

        return {
            'sentiment': sentiment,
            'confidence': confidence,
            'scores': {
                'basic': {
                    'positive_score': positive_score,
                    'negative_score': negative_score,
                    'polarity': polarity,
                    'total_words': len(words),
                    'sentiment_words': len(sentiment_details)
                }
            },
            'available': True,
            'methods_used': ['enhanced_basic_dictionary'],
            'details': sentiment_details[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ªæƒ…æ„Ÿè¯
        }

    def analyze_syntax(self, text: Optional[str] = None, max_length: int = 5000) -> Dict:
        """
        ä¾å­˜å¥æ³•åˆ†æåŠŸèƒ½ï¼ˆæ”¯æŒå¤§æ–‡æœ¬åˆ†å—å¤„ç†ï¼‰

        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ–‡æœ¬
            max_length: å•æ¬¡å¤„ç†çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œè¶…è¿‡åˆ™åˆ†å—å¤„ç†

        Returns:
            {
                'sentences': [å¥æ³•åˆ†æç»“æœ],
                'available': æ˜¯å¦å¯ç”¨,
                'model_used': ä½¿ç”¨çš„æ¨¡å‹,
                'is_truncated': æ˜¯å¦è¢«æˆªæ–­
            }
        """
        if text is None:
            text = self.text

        if not text.strip():
            return {'sentences': [], 'available': False, 'model_used': None, 'is_truncated': False}

        # å¯¹äºå¤§æ–‡æœ¬ï¼Œåªåˆ†æå‰é¢éƒ¨åˆ†ä»¥æé«˜æ€§èƒ½
        is_truncated = False
        if len(text) > max_length:
            text = text[:max_length]
            is_truncated = True

        # ä½¿ç”¨Stanzaè¿›è¡Œå¥æ³•åˆ†æ
        if 'stanza_zh' in self.nlp_models:
            try:
                doc = self.nlp_models['stanza_zh'](text)
                sentences = []
                for sent in doc.sentences:
                    words = []
                    for word in sent.words:
                        words.append({
                            'text': word.text,
                            'lemma': word.lemma,
                            'pos': word.upos,
                            'head': word.head,
                            'deprel': word.deprel
                        })
                    sentences.append({
                        'text': sent.text,
                        'words': words
                    })
                return {
                    'sentences': sentences,
                    'available': True,
                    'model_used': 'stanza_zh',
                    'is_truncated': is_truncated
                }
            except Exception:
                pass

        elif 'stanza_en' in self.nlp_models:
            try:
                doc = self.nlp_models['stanza_en'](text)
                sentences = []
                for sent in doc.sentences:
                    words = []
                    for word in sent.words:
                        words.append({
                            'text': word.text,
                            'lemma': word.lemma,
                            'pos': word.upos,
                            'head': word.head,
                            'deprel': word.deprel
                        })
                    sentences.append({
                        'text': sent.text,
                        'words': words
                    })
                return {
                    'sentences': sentences,
                    'available': True,
                    'model_used': 'stanza_en',
                    'is_truncated': is_truncated
                }
            except Exception:
                pass

        # ä½¿ç”¨åŸºç¡€å¥æ³•åˆ†æ
        result = self._basic_syntax_analysis(text)
        result['is_truncated'] = is_truncated
        return result

    def _basic_syntax_analysis(self, text: str) -> Dict:
        """åŸºç¡€å¥æ³•åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        sentences = self._split_sentences(text)
        analyzed_sentences = []

        for sentence in sentences:
            words = sentence.split()
            analyzed_words = []

            for i, word in enumerate(words):
                # ç®€å•çš„è¯æ€§å’Œä¾å­˜å…³ç³»æ¨æ–­
                pos = self._guess_pos(word)
                deprel = self._guess_deprel(word, i, len(words))

                analyzed_words.append({
                    'text': word,
                    'lemma': word.lower(),  # ç®€åŒ–çš„è¯æ ¹
                    'pos': pos,
                    'head': 0,  # ç®€åŒ–å¤„ç†
                    'deprel': deprel
                })

            analyzed_sentences.append({
                'text': sentence,
                'words': analyzed_words
            })

        return {
            'sentences': analyzed_sentences,
            'available': True,
            'model_used': 'basic_rules'
        }

    def _guess_pos(self, word: str) -> str:
        """ç®€å•çš„è¯æ€§æ¨æ–­"""
        # åŸºäºç®€å•è§„åˆ™çš„è¯æ€§æ¨æ–­
        if re.match(r'\d+', word):
            return 'NUM'
        elif word in ['çš„', 'äº†', 'ç€', 'è¿‡', 'the', 'a', 'an']:
            return 'DET'
        elif word in ['å’Œ', 'ä¸', 'and', 'or']:
            return 'CCONJ'
        elif word in ['åœ¨', 'in', 'on', 'at']:
            return 'ADP'
        else:
            return 'NOUN'  # é»˜è®¤ä¸ºåè¯

    def _guess_deprel(self, word: str, position: int, total_words: int) -> str:
        """ç®€å•çš„ä¾å­˜å…³ç³»æ¨æ–­"""
        if position == 0:
            return 'nsubj'  # ç¬¬ä¸€ä¸ªè¯é€šå¸¸æ˜¯ä¸»è¯­
        elif position == total_words - 1:
            return 'obj'    # æœ€åä¸€ä¸ªè¯é€šå¸¸æ˜¯å®¾è¯­
        elif position == 1 or 'æ˜¯' in word or 'is' in word.lower():
            return 'root'   # è°“è¯­
        else:
            return 'amod'   # ä¿®é¥°è¯­

    def generate_enhanced_summary(self, num_sentences: int = 3,
                                method: str = 'enhanced_hybrid') -> str:
        """
        å¢å¼ºçš„æ–‡æœ¬æ‘˜è¦ç”Ÿæˆï¼ˆé›†æˆå¥æ³•åˆ†æï¼‰

        Args:
            num_sentences: æ‘˜è¦å¥å­æ•°
            method: æ‘˜è¦æ–¹æ³• ('enhanced_hybrid', 'syntax_based', æˆ–åŸæœ‰æ–¹æ³•)

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        if method in ['frequency', 'position', 'hybrid']:
            # ä½¿ç”¨åŸæœ‰æ–¹æ³•
            return self.generate_summary(num_sentences, method)

        sentences = self._split_sentences(self.text)

        if len(sentences) <= num_sentences:
            return self.text

        if method == 'enhanced_hybrid':
            return self._enhanced_hybrid_summary(sentences, num_sentences)
        elif method == 'syntax_based':
            return self._syntax_based_summary(sentences, num_sentences)
        else:
            # é»˜è®¤ä½¿ç”¨å¢å¼ºæ··åˆæ–¹æ³•
            return self._enhanced_hybrid_summary(sentences, num_sentences)

    def intelligent_rewrite(self, style: str = 'formal', intensity: str = 'medium',
                          segment_mode: bool = True, max_segment_length: int = 1000) -> str:
        """
        æ™ºèƒ½æ”¹å†™æ–‡æœ¬ï¼ˆæ”¯æŒåˆ†æ®µå¤„ç†ï¼‰

        Args:
            style: æ”¹å†™é£æ ¼ ('formal', 'casual', 'academic', 'creative', 'concise')
            intensity: æ”¹å†™å¼ºåº¦ ('light', 'medium', 'heavy')
            segment_mode: æ˜¯å¦å¯ç”¨åˆ†æ®µæ¨¡å¼ï¼ˆæ¨èé•¿æ–‡æœ¬ä½¿ç”¨ï¼‰
            max_segment_length: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°

        Returns:
            æ”¹å†™åçš„æ–‡æœ¬
        """
        if not self.text:
            return ""

        # å¦‚æœæ–‡æœ¬è¾ƒçŸ­æˆ–ä¸å¯ç”¨åˆ†æ®µæ¨¡å¼ï¼Œç›´æ¥å¤„ç†
        if not segment_mode or len(self.text) <= max_segment_length:
            if self.qwen3_client:
                return self._qwen3_rewrite(style, intensity)
            else:
                return self._basic_rewrite(style, intensity)

        # é•¿æ–‡æœ¬åˆ†æ®µå¤„ç†
        return self._segmented_rewrite(style, intensity, max_segment_length)

    def _enhanced_hybrid_summary(self, sentences: List[str], num_sentences: int) -> str:
        """å¢å¼ºçš„æ··åˆæ–¹æ³•æ‘˜è¦ï¼ˆç»“åˆå¥æ³•åˆ†æï¼‰"""
        word_freq = self.word_frequency(ignore_case=True, exclude_punctuation=True)
        total_sentences = len(sentences)

        sentence_scores = []
        for i, sentence in enumerate(sentences):
            # åŸºç¡€è¯é¢‘åˆ†æ•°
            words = sentence.lower().translate(str.maketrans('', '', string.punctuation)).split()
            freq_score = sum(word_freq.get(word, 0) for word in words)

            # ä½ç½®æƒé‡
            if i == 0 or i == total_sentences - 1:
                position_weight = 1.5
            elif i < total_sentences * 0.2 or i > total_sentences * 0.8:
                position_weight = 1.2
            else:
                position_weight = 1.0

            # å¥æ³•å¤æ‚åº¦æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            syntax_weight = self._calculate_syntax_weight(sentence)

            # å®ä½“æƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            entity_weight = self._calculate_entity_weight(sentence)

            final_score = freq_score * position_weight * syntax_weight * entity_weight
            sentence_scores.append((final_score, sentence))

        sentence_scores.sort(reverse=True)
        top_sentences = [sent for _, sent in sentence_scores[:num_sentences]]

        return '. '.join(top_sentences) + '.'

    def _syntax_based_summary(self, sentences: List[str], num_sentences: int) -> str:
        """åŸºäºå¥æ³•åˆ†æçš„æ‘˜è¦"""
        sentence_scores = []

        for sentence in sentences:
            syntax_analysis = self.analyze_syntax(sentence)
            score = 0

            if syntax_analysis['available']:
                # æ ¹æ®å¥æ³•ç»“æ„è®¡ç®—åˆ†æ•°
                for sent_data in syntax_analysis['sentences']:
                    for word in sent_data['words']:
                        # ä¸»è¯­ã€è°“è¯­ã€å®¾è¯­æƒé‡æ›´é«˜
                        if word['deprel'] in ['nsubj', 'root', 'obj', 'dobj']:
                            score += 2
                        elif word['deprel'] in ['amod', 'compound']:
                            score += 1
            else:
                # å¦‚æœå¥æ³•åˆ†æä¸å¯ç”¨ï¼Œä½¿ç”¨å¥å­é•¿åº¦ä½œä¸ºç®€å•æŒ‡æ ‡
                score = len(sentence.split())

            sentence_scores.append((score, sentence))

        sentence_scores.sort(reverse=True)
        top_sentences = [sent for _, sent in sentence_scores[:num_sentences]]

        return '. '.join(top_sentences) + '.'

    def _calculate_syntax_weight(self, sentence: str) -> float:
        """è®¡ç®—å¥æ³•å¤æ‚åº¦æƒé‡"""
        syntax_analysis = self.analyze_syntax(sentence)

        if not syntax_analysis['available']:
            return 1.0

        weight = 1.0
        for sent_data in syntax_analysis['sentences']:
            # åŒ…å«æ›´å¤šé‡è¦å¥æ³•å…³ç³»çš„å¥å­æƒé‡æ›´é«˜
            important_relations = ['nsubj', 'root', 'obj', 'dobj', 'amod']
            relation_count = sum(1 for word in sent_data['words']
                               if word['deprel'] in important_relations)
            if relation_count > 3:
                weight += 0.2

        return weight

    def _calculate_entity_weight(self, sentence: str) -> float:
        """è®¡ç®—å®ä½“æƒé‡"""
        entities = self.extract_entities(sentence)

        if not entities['available']:
            return 1.0

        # åŒ…å«æ›´å¤šå®ä½“çš„å¥å­æƒé‡æ›´é«˜
        entity_count = len(entities['entities'])
        if entity_count > 0:
            return 1.0 + (entity_count * 0.1)

        return 1.0

    def get_nlp_capabilities(self) -> Dict:
        """è·å–å½“å‰å¯ç”¨çš„NLPåŠŸèƒ½"""
        # æ£€æŸ¥å¯ç”¨çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•
        sentiment_methods = []
        if self.nlp_models.get('vader'):
            sentiment_methods.append('vader')
        if TEXTBLOB_AVAILABLE:
            sentiment_methods.append('textblob')
        if self.nlp_models.get('snownlp'):
            sentiment_methods.append('snownlp')

        # æ£€æŸ¥æ·±åº¦å­¦ä¹ æ¨¡å‹
        dl_models = ['erlangshen_roberta_330m', 'erlangshen_roberta_110m', 'chinese_roberta_wwm_ext']
        available_dl_models = [model for model in dl_models if model in self.nlp_models]
        if available_dl_models:
            sentiment_methods.extend(available_dl_models)

        # åŸºç¡€æ–¹æ³•æ€»æ˜¯å¯ç”¨
        sentiment_methods.append('basic_dictionary')

        return {
            'entity_recognition': {
                'available': True,
                'methods': ['basic_regex']
            },
            'sentiment_analysis': {
                'available': True,
                'methods': sentiment_methods
            },
            'syntax_analysis': {
                'available': True,
                'methods': ['basic'] + ([model for model in ['stanza_zh', 'stanza_en'] if model in self.nlp_models])
            },
            'enhanced_summary': True,
            'textteaser_summary': bool(self.textteaser),
            'qwen3_summary': bool(self.qwen3_client),
            'intelligent_rewrite': True,
            'qwen3_rewrite': bool(self.qwen3_client),
            'advanced_entity_recognition': bool(self.nlp_models.get('spacy_zh') or self.nlp_models.get('spacy_en')),
            'advanced_sentiment_analysis': len(sentiment_methods) > 1,
            'advanced_syntax_analysis': bool(self.nlp_models.get('stanza_zh') or self.nlp_models.get('stanza_en'))
        }

    def _qwen3_rewrite(self, style: str, intensity: str) -> str:
        """ä½¿ç”¨Qwen3æ¨¡å‹è¿›è¡Œæ™ºèƒ½æ”¹å†™"""
        try:
            # è¯»å–æç¤ºè¯æ–‡ä»¶
            prompt_content = self._load_rewrite_prompt()

            # æ„å»ºæ”¹å†™é£æ ¼æè¿°
            style_descriptions = {
                'formal': 'æ­£å¼é£æ ¼ï¼ˆFormalï¼‰- ä½¿ç”¨æ­£å¼ã€ä¸¥è°¨çš„è¯­è¨€ï¼Œé€‚åˆå•†åŠ¡æ–‡æ¡£ã€å®˜æ–¹æŠ¥å‘Šç­‰åœºåˆ',
                'casual': 'å£è¯­åŒ–ï¼ˆCasual/Spokenï¼‰- ä½¿ç”¨è½»æ¾ã€è‡ªç„¶çš„å£è¯­è¡¨è¾¾ï¼Œè´´è¿‘æ—¥å¸¸å¯¹è¯',
                'academic': 'å­¦æœ¯é£æ ¼ï¼ˆAcademicï¼‰- ä½¿ç”¨ä¸“ä¸šã€ä¸¥è°¨çš„å­¦æœ¯è¯­è¨€ï¼Œé€‚åˆå­¦æœ¯è®ºæ–‡ã€ç ”ç©¶æŠ¥å‘Š',
                'creative': 'æ–‡å­¦åŒ–ï¼ˆLiteraryï¼‰- ä½¿ç”¨å¯Œæœ‰åˆ›æ„ã€ç”ŸåŠ¨å½¢è±¡çš„æ–‡å­¦è¡¨è¾¾',
                'concise': 'ç®€æ´å‡ç»ƒï¼ˆConciseï¼‰- ä½¿ç”¨ç²¾ç‚¼ã€ç®€æ´çš„è¯­è¨€ï¼Œå»é™¤å†—ä½™è¡¨è¾¾'
            }

            # æ„å»ºæ”¹å†™å¼ºåº¦æè¿°
            intensity_descriptions = {
                'light': 'è½»åº¦æ”¹å†™ - ä¿æŒåŸæ–‡çš„å¥å¼ç»“æ„ï¼Œä¸»è¦è°ƒæ•´ç”¨è¯å’Œè¡¨è¾¾æ–¹å¼ï¼Œç¡®ä¿è¯­è¨€é£æ ¼çš„è½¬æ¢',
                'medium': 'ä¸­åº¦æ”¹å†™ - é€‚åº¦è°ƒæ•´å¥å¼ç»“æ„å’Œè¡¨è¾¾æ–¹å¼ï¼Œåœ¨ä¿æŒåŸæ„çš„åŸºç¡€ä¸Šè¿›è¡Œè¾ƒå¤§ç¨‹åº¦çš„è¯­è¨€é£æ ¼è½¬æ¢',
                'heavy': 'é‡åº¦æ”¹å†™ - å¤§å¹…æ”¹å†™å¥å¼å’Œè¡¨è¾¾æ–¹å¼ï¼Œç”¨å®Œå…¨ä¸åŒçš„è¯­è¨€é£æ ¼é‡æ–°è¡¨è¾¾ç›¸åŒçš„å†…å®¹'
            }

            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            content = f"""{prompt_content}

æ”¹å†™ä»»åŠ¡ï¼š
è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ”¹å†™ä¸ºï¼š{style_descriptions.get(style, style_descriptions['formal'])}

æ”¹å†™å¼ºåº¦ï¼š{intensity_descriptions.get(intensity, intensity_descriptions['medium'])}

æ”¹å†™è¦æ±‚ï¼š
1. ä¸¥æ ¼ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€å’Œæ‰€æœ‰é‡è¦ä¿¡æ¯
2. æ”¹å†™åçš„æ–‡æœ¬å¿…é¡»æµç•…è‡ªç„¶ï¼Œç¬¦åˆç›®æ ‡è¯­è¨€é£æ ¼
3. æ ¹æ®æŒ‡å®šçš„æ”¹å†™å¼ºåº¦è¿›è¡Œç›¸åº”ç¨‹åº¦çš„è°ƒæ•´
4. ç›´æ¥è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ‡è®°
5. ä¿æŒæ–‡æœ¬çš„é€»è¾‘ç»“æ„å’Œæ®µè½åˆ’åˆ†

åŸå§‹æ–‡æœ¬ï¼š
{self.text}

æ”¹å†™åçš„æ–‡æœ¬ï¼š"""

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.qwen3_model,
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "stream": False
            }

            # è°ƒç”¨Qwen3æ¨¡å‹API
            headers = {'Content-type': 'application/json'}
            response = requests.post(self.qwen3_api_url,
                                   data=json.dumps(data),
                                   headers=headers,
                                   timeout=60)

            if response.status_code == 200:
                response_data = response.json()
                rewritten_text = response_data["message"]["content"].strip()

                # æ¸…ç†è¾“å‡º
                rewritten_text = self._clean_rewrite_output(rewritten_text)

                return rewritten_text
            else:
                print(f"Qwen3 APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return self._basic_rewrite(style, intensity)

        except Exception as e:
            print(f"Qwen3æ”¹å†™å¤±è´¥: {e}")
            # é™çº§åˆ°åŸºç¡€æ”¹å†™æ–¹æ³•
            return self._basic_rewrite(style, intensity)

    def _basic_rewrite(self, style: str, intensity: str) -> str:
        """åŸºç¡€æ”¹å†™æ–¹æ³•"""
        try:
            sentences = self._split_sentences(self.text)
            rewritten_sentences = []

            for sentence in sentences:
                if not sentence.strip():
                    continue

                rewritten_sentence = self._rewrite_sentence(sentence, style, intensity)
                rewritten_sentences.append(rewritten_sentence)

            return ''.join(rewritten_sentences)

        except Exception as e:
            print(f"åŸºç¡€æ”¹å†™å¤±è´¥: {e}")
            return self.text

    def _rewrite_sentence(self, sentence: str, style: str, intensity: str) -> str:
        """æ”¹å†™å•ä¸ªå¥å­"""
        # åŸºç¡€çš„å¥å­æ”¹å†™é€»è¾‘
        rewritten = sentence

        # æ ¹æ®é£æ ¼è°ƒæ•´
        if style == 'formal':
            # æ­£å¼é£æ ¼ï¼šä½¿ç”¨æ›´æ­£å¼çš„è¯æ±‡
            replacements = {
                'å¾ˆ': 'éå¸¸',
                'æŒº': 'ç›¸å½“',
                'ç‰¹åˆ«': 'å°¤å…¶',
                'çœŸçš„': 'ç¡®å®',
                'å¥½çš„': 'è‰¯å¥½çš„',
                'ä¸é”™': 'ä¼˜ç§€',
                'å‰å®³': 'å‡ºè‰²'
            }
        elif style == 'casual':
            # è½»æ¾é£æ ¼ï¼šä½¿ç”¨æ›´å£è¯­åŒ–çš„è¡¨è¾¾
            replacements = {
                'éå¸¸': 'å¾ˆ',
                'ç›¸å½“': 'æŒº',
                'å°¤å…¶': 'ç‰¹åˆ«',
                'ç¡®å®': 'çœŸçš„',
                'è‰¯å¥½çš„': 'å¥½çš„',
                'ä¼˜ç§€': 'ä¸é”™',
                'å‡ºè‰²': 'å‰å®³'
            }
        elif style == 'academic':
            # å­¦æœ¯é£æ ¼ï¼šä½¿ç”¨æ›´ä¸“ä¸šçš„è¯æ±‡
            replacements = {
                'æ˜¾ç¤º': 'è¡¨æ˜',
                'è¯´æ˜': 'é˜è¿°',
                'å› ä¸º': 'ç”±äº',
                'æ‰€ä»¥': 'å› æ­¤',
                'ä½†æ˜¯': 'ç„¶è€Œ',
                'è€Œä¸”': 'æ­¤å¤–'
            }
        elif style == 'creative':
            # åˆ›æ„é£æ ¼ï¼šä½¿ç”¨æ›´ç”ŸåŠ¨çš„è¡¨è¾¾
            replacements = {
                'å¾ˆå¤§': 'å·¨å¤§',
                'å¾ˆå°': 'å¾®å°',
                'å¾ˆå¿«': 'è¿…é€Ÿ',
                'å¾ˆæ…¢': 'ç¼“æ…¢',
                'å¾ˆå¥½': 'ç»ä½³',
                'å¾ˆå·®': 'ç³Ÿç³•'
            }
        elif style == 'concise':
            # ç®€æ´é£æ ¼ï¼šå»é™¤å†—ä½™è¯æ±‡
            replacements = {
                'éå¸¸çš„': '',
                'ååˆ†çš„': '',
                'ç›¸å½“çš„': '',
                'æ¯”è¾ƒçš„': '',
                'æœ‰ä¸€äº›': 'ä¸€äº›',
                'è¿›è¡Œäº†': '',
                'å®æ–½äº†': ''
            }
        else:
            replacements = {}

        # åº”ç”¨æ›¿æ¢
        for old, new in replacements.items():
            rewritten = rewritten.replace(old, new)

        # æ ¹æ®å¼ºåº¦è°ƒæ•´
        if intensity == 'heavy':
            # é‡åº¦æ”¹å†™ï¼šå°è¯•æ”¹å˜å¥å¼ç»“æ„
            rewritten = self._restructure_sentence(rewritten)

        return rewritten

    def _restructure_sentence(self, sentence: str) -> str:
        """é‡æ„å¥å­ç»“æ„"""
        # ç®€å•çš„å¥å¼å˜æ¢
        if 'ï¼Œ' in sentence:
            parts = sentence.split('ï¼Œ')
            if len(parts) == 2:
                # å°è¯•é¢ å€’é¡ºåº
                return f"{parts[1]}ï¼Œ{parts[0]}"

        return sentence

    def _clean_rewrite_output(self, text: str) -> str:
        """æ¸…ç†æ”¹å†™è¾“å‡º"""
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€
        prefixes_to_remove = [
            "æ”¹å†™åçš„æ–‡æœ¬ï¼š",
            "æ”¹å†™ç»“æœï¼š",
            "é‡å†™åï¼š",
            "ä¿®æ”¹åï¼š",
            "æ”¹å†™ï¼š",
            "ç»“æœï¼š"
        ]

        for prefix in prefixes_to_remove:
            if text.startswith(prefix):
                text = text[len(prefix):].strip()

        # ç§»é™¤å¤šä½™çš„å¼•å·
        text = text.strip('"').strip("'").strip()

        text = re.sub(
                        r'<think>.*?</think>',  # éè´ªå©ªåŒ¹é…ä»»æ„å­—ç¬¦
                        '',                     # æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                        text, 
                        flags=re.DOTALL          # ä½¿.åŒ¹é…æ¢è¡Œç¬¦
                    )

        return text

    def _segmented_rewrite(self, style: str, intensity: str, max_segment_length: int) -> str:
        """
        åˆ†æ®µæ”¹å†™é•¿æ–‡æœ¬

        Args:
            style: æ”¹å†™é£æ ¼
            intensity: æ”¹å†™å¼ºåº¦
            max_segment_length: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°

        Returns:
            æ”¹å†™åçš„å®Œæ•´æ–‡æœ¬
        """
        try:
            # æ™ºèƒ½åˆ†æ®µï¼šä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²ï¼Œç„¶åæŒ‰å¥å­åˆ†å‰²
            segments = self._smart_segment_text(self.text, max_segment_length)

            print(f"æ–‡æœ¬åˆ†ä¸º {len(segments)} æ®µè¿›è¡Œæ”¹å†™...")

            rewritten_segments = []

            for i, segment in enumerate(segments):
                if not segment.strip():
                    rewritten_segments.append(segment)
                    continue

                print(f"æ­£åœ¨æ”¹å†™ç¬¬ {i+1}/{len(segments)} æ®µ...")

                # ä¸ºæ¯ä¸ªæ®µè½åˆ›å»ºä¸´æ—¶å¤„ç†å™¨å®ä¾‹æˆ–ç›´æ¥å¤„ç†
                if self.qwen3_client:
                    rewritten_segment = self._qwen3_rewrite_segment(segment, style, intensity, i+1, len(segments))
                    rewritten_segment = re.sub(
                        r'<think>.*?</think>',  # éè´ªå©ªåŒ¹é…ä»»æ„å­—ç¬¦
                        '',                     # æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                        rewritten_segment, 
                        flags=re.DOTALL          # ä½¿.åŒ¹é…æ¢è¡Œç¬¦
                    )
                else:
                    rewritten_segment = self._basic_rewrite_segment(segment, style, intensity)

                rewritten_segments.append(rewritten_segment)

                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡äºé¢‘ç¹
                if self.qwen3_client and i < len(segments) - 1:
                    import time
                    time.sleep(0.5)

            # åˆå¹¶æ‰€æœ‰æ”¹å†™åçš„æ®µè½
            result = ''.join(rewritten_segments)
            print("åˆ†æ®µæ”¹å†™å®Œæˆï¼")

            return result

        except Exception as e:
            print(f"åˆ†æ®µæ”¹å†™å¤±è´¥: {e}")
            # é™çº§åˆ°åŸºç¡€æ”¹å†™
            return self._basic_rewrite(style, intensity)

    def _smart_segment_text(self, text: str, max_length: int) -> List[str]:
        """
        æ™ºèƒ½åˆ†æ®µï¼šä¼˜å…ˆä¿æŒæ®µè½å’Œå¥å­çš„å®Œæ•´æ€§

        Args:
            text: è¦åˆ†æ®µçš„æ–‡æœ¬
            max_length: æ¯æ®µæœ€å¤§é•¿åº¦

        Returns:
            åˆ†æ®µåçš„æ–‡æœ¬åˆ—è¡¨
        """
        if len(text) <= max_length:
            return [text]

        segments = []

        # é¦–å…ˆæŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²æ®µè½
        paragraphs = text.split('\n\n')

        current_segment = ""

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥æ·»åŠ 
            if len(current_segment + '\n\n' + paragraph) <= max_length:
                if current_segment:
                    current_segment += '\n\n' + paragraph
                else:
                    current_segment = paragraph
            else:
                # å¦‚æœå½“å‰æ®µè½æœ‰å†…å®¹ï¼Œå…ˆä¿å­˜
                if current_segment:
                    segments.append(current_segment)
                    current_segment = ""

                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦æŒ‰å¥å­åˆ†å‰²
                if len(paragraph) > max_length:
                    sentence_segments = self._segment_by_sentences(paragraph, max_length)
                    segments.extend(sentence_segments)
                else:
                    current_segment = paragraph

        # æ·»åŠ æœ€åä¸€æ®µ
        if current_segment:
            segments.append(current_segment)

        return segments

    def _segment_by_sentences(self, text: str, max_length: int) -> List[str]:
        """
        æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬

        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            max_length: æ¯æ®µæœ€å¤§é•¿åº¦

        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        sentences = self._split_sentences(text)
        segments = []
        current_segment = ""

        for sentence in sentences:
            if not sentence.strip():
                continue

            # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡é™åˆ¶ï¼Œå¼ºåˆ¶åˆ†å‰²
            if len(sentence) > max_length:
                if current_segment:
                    segments.append(current_segment)
                    current_segment = ""

                # æŒ‰å­—ç¬¦å¼ºåˆ¶åˆ†å‰²
                for i in range(0, len(sentence), max_length):
                    segments.append(sentence[i:i+max_length])
            else:
                # æ£€æŸ¥æ·»åŠ è¿™ä¸ªå¥å­æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
                if len(current_segment + sentence) <= max_length:
                    current_segment += sentence
                else:
                    if current_segment:
                        segments.append(current_segment)
                    current_segment = sentence

        if current_segment:
            segments.append(current_segment)

        return segments

    def _qwen3_rewrite_segment(self, segment: str, style: str, intensity: str,
                              segment_num: int, total_segments: int) -> str:
        """
        ä½¿ç”¨Qwen3æ¨¡å‹æ”¹å†™å•ä¸ªæ–‡æœ¬æ®µè½

        Args:
            segment: è¦æ”¹å†™çš„æ–‡æœ¬æ®µè½
            style: æ”¹å†™é£æ ¼
            intensity: æ”¹å†™å¼ºåº¦
            segment_num: å½“å‰æ®µè½ç¼–å·
            total_segments: æ€»æ®µè½æ•°

        Returns:
            æ”¹å†™åçš„æ®µè½
        """
        try:
            # è¯»å–æç¤ºè¯æ–‡ä»¶
            prompt_content = self._load_rewrite_prompt()

            # æ„å»ºæ”¹å†™é£æ ¼æè¿°
            style_descriptions = {
                'formal': 'æ­£å¼é£æ ¼ï¼ˆFormalï¼‰- ä½¿ç”¨æ­£å¼ã€ä¸¥è°¨çš„è¯­è¨€ï¼Œé€‚åˆå•†åŠ¡æ–‡æ¡£ã€å®˜æ–¹æŠ¥å‘Šç­‰åœºåˆ',
                'casual': 'å£è¯­åŒ–ï¼ˆCasual/Spokenï¼‰- ä½¿ç”¨è½»æ¾ã€è‡ªç„¶çš„å£è¯­è¡¨è¾¾ï¼Œè´´è¿‘æ—¥å¸¸å¯¹è¯',
                'academic': 'å­¦æœ¯é£æ ¼ï¼ˆAcademicï¼‰- ä½¿ç”¨ä¸“ä¸šã€ä¸¥è°¨çš„å­¦æœ¯è¯­è¨€ï¼Œé€‚åˆå­¦æœ¯è®ºæ–‡ã€ç ”ç©¶æŠ¥å‘Š',
                'creative': 'æ–‡å­¦åŒ–ï¼ˆLiteraryï¼‰- ä½¿ç”¨å¯Œæœ‰åˆ›æ„ã€ç”ŸåŠ¨å½¢è±¡çš„æ–‡å­¦è¡¨è¾¾',
                'concise': 'ç®€æ´å‡ç»ƒï¼ˆConciseï¼‰- ä½¿ç”¨ç²¾ç‚¼ã€ç®€æ´çš„è¯­è¨€ï¼Œå»é™¤å†—ä½™è¡¨è¾¾'
            }

            # æ„å»ºæ”¹å†™å¼ºåº¦æè¿°
            intensity_descriptions = {
                'light': 'è½»åº¦æ”¹å†™ - ä¿æŒåŸæ–‡çš„å¥å¼ç»“æ„ï¼Œä¸»è¦è°ƒæ•´ç”¨è¯å’Œè¡¨è¾¾æ–¹å¼ï¼Œç¡®ä¿è¯­è¨€é£æ ¼çš„è½¬æ¢',
                'medium': 'ä¸­åº¦æ”¹å†™ - é€‚åº¦è°ƒæ•´å¥å¼ç»“æ„å’Œè¡¨è¾¾æ–¹å¼ï¼Œåœ¨ä¿æŒåŸæ„çš„åŸºç¡€ä¸Šè¿›è¡Œè¾ƒå¤§ç¨‹åº¦çš„è¯­è¨€é£æ ¼è½¬æ¢',
                'heavy': 'é‡åº¦æ”¹å†™ - å¤§å¹…æ”¹å†™å¥å¼å’Œè¡¨è¾¾æ–¹å¼ï¼Œç”¨å®Œå…¨ä¸åŒçš„è¯­è¨€é£æ ¼é‡æ–°è¡¨è¾¾ç›¸åŒçš„å†…å®¹'
            }

            # æ„å»ºåˆ†æ®µæ”¹å†™çš„ç‰¹æ®Šæç¤ºè¯
            content = f"""{prompt_content}

æ”¹å†™ä»»åŠ¡ï¼š
è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ®µè½æ”¹å†™ä¸ºï¼š{style_descriptions.get(style, style_descriptions['formal'])}

æ”¹å†™å¼ºåº¦ï¼š{intensity_descriptions.get(intensity, intensity_descriptions['medium'])}

ç‰¹åˆ«è¯´æ˜ï¼š
- è¿™æ˜¯ç¬¬ {segment_num} æ®µï¼Œå…± {total_segments} æ®µ
- è¯·ä¿æŒæ®µè½çš„ç‹¬ç«‹æ€§å’Œå®Œæ•´æ€§
- ç¡®ä¿æ”¹å†™åçš„æ®µè½èƒ½å¤Ÿä¸å…¶ä»–æ®µè½è‡ªç„¶è¡”æ¥
- ä¿æŒæ®µè½å†…éƒ¨çš„é€»è¾‘ç»“æ„

æ”¹å†™è¦æ±‚ï¼š
1. ä¸¥æ ¼ä¿æŒåŸæ–‡çš„æ ¸å¿ƒæ„æ€å’Œæ‰€æœ‰é‡è¦ä¿¡æ¯
2. æ”¹å†™åçš„æ–‡æœ¬å¿…é¡»æµç•…è‡ªç„¶ï¼Œç¬¦åˆç›®æ ‡è¯­è¨€é£æ ¼
3. æ ¹æ®æŒ‡å®šçš„æ”¹å†™å¼ºåº¦è¿›è¡Œç›¸åº”ç¨‹åº¦çš„è°ƒæ•´
4. ç›´æ¥è¾“å‡ºæ”¹å†™åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€è¯´æ˜æˆ–æ ‡è®°
5. ä¿æŒæ®µè½çš„é€»è¾‘ç»“æ„å’Œæ ¼å¼

åŸå§‹æ–‡æœ¬æ®µè½ï¼š
{segment}

æ”¹å†™åçš„æ–‡æœ¬æ®µè½ï¼š"""

            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self.qwen3_model,
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "stream": False
            }

            # è°ƒç”¨Qwen3æ¨¡å‹API
            headers = {'Content-type': 'application/json'}
            response = requests.post(self.qwen3_api_url,
                                   data=json.dumps(data),
                                   headers=headers,
                                   timeout=60)

            if response.status_code == 200:
                response_data = response.json()
                rewritten_text = response_data["message"]["content"].strip()

                # æ¸…ç†è¾“å‡º
                rewritten_text = self._clean_rewrite_output(rewritten_text)

                return rewritten_text
            else:
                print(f"Qwen3 APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return self._basic_rewrite_segment(segment, style, intensity)

        except Exception as e:
            print(f"Qwen3æ®µè½æ”¹å†™å¤±è´¥: {e}")
            # é™çº§åˆ°åŸºç¡€æ”¹å†™æ–¹æ³•
            return self._basic_rewrite_segment(segment, style, intensity)

    def _basic_rewrite_segment(self, segment: str, style: str, intensity: str) -> str:
        """
        åŸºç¡€æ–¹æ³•æ”¹å†™å•ä¸ªæ–‡æœ¬æ®µè½

        Args:
            segment: è¦æ”¹å†™çš„æ–‡æœ¬æ®µè½
            style: æ”¹å†™é£æ ¼
            intensity: æ”¹å†™å¼ºåº¦

        Returns:
            æ”¹å†™åçš„æ®µè½
        """
        try:
            sentences = self._split_sentences(segment)
            rewritten_sentences = []

            for sentence in sentences:
                if not sentence.strip():
                    continue

                rewritten_sentence = self._rewrite_sentence(sentence, style, intensity)
                rewritten_sentences.append(rewritten_sentence)

            return ''.join(rewritten_sentences)

        except Exception as e:
            print(f"åŸºç¡€æ®µè½æ”¹å†™å¤±è´¥: {e}")
            return segment

    def _load_rewrite_prompt(self) -> str:
        """åŠ è½½æ”¹å†™æç¤ºè¯"""
        try:
            # å°è¯•ä»å¤šä¸ªå¯èƒ½çš„è·¯å¾„è¯»å–prompt.txt
            possible_paths = [
                'prompt.txt',
                '../prompt.txt',
                '../../prompt.txt',
                os.path.join(os.path.dirname(os.path.dirname(__file__)), 'prompt.txt')
            ]

            for path in possible_paths:
                try:
                    if os.path.exists(path):
                        with open(path, 'r', encoding='utf-8') as f:
                            return f.read().strip()
                except Exception:
                    continue

            # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œè¿”å›é»˜è®¤æç¤ºè¯
            return """è§’è‰²è®¾å®šï¼š
ä½ æ˜¯ä¸€ä½è¯­è¨€é£æ ¼ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®ä¸åŒéœ€æ±‚å¯¹æ–‡æœ¬è¿›è¡Œæ”¹å†™ã€‚ä½ ç†è§£å„ç§è¯­è¨€é£æ ¼ä¹‹é—´çš„åŒºåˆ«ï¼Œå¹¶èƒ½ä¿ç•™åŸæ–‡çš„æ ¸å¿ƒå«ä¹‰ï¼Œåœ¨ä¸åŒé£æ ¼ä¸‹è¿›è¡Œè‡ªç„¶ã€æµç•…ã€ç¬¦åˆè¯­å¢ƒçš„è¡¨è¾¾ã€‚

ä»»åŠ¡è¦æ±‚ï¼š
ç”¨æˆ·å°†æä¾›ä¸€æ®µåŸå§‹æ–‡æœ¬ï¼Œå¹¶æŒ‡å®šå¸Œæœ›çš„æ”¹å†™é£æ ¼ï¼Œä½ éœ€è¦åœ¨ä¿æŒåŸæ„çš„åŸºç¡€ä¸Šï¼Œå¯¹æ–‡æœ¬è¿›è¡Œæ”¹å†™ï¼Œä½¿å…¶é£æ ¼æ›´è´´è¿‘ç”¨æˆ·çš„è¦æ±‚ã€‚"""

        except Exception as e:
            print(f"åŠ è½½æç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}")
            return "ä½ æ˜¯ä¸€ä½è¯­è¨€é£æ ¼ä¸“å®¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·è¦æ±‚å¯¹æ–‡æœ¬è¿›è¡Œæ”¹å†™ã€‚"

    def _is_offline_mode(self) -> bool:
        """æ£€æµ‹æ˜¯å¦å¤„äºç¦»çº¿æ¨¡å¼ï¼ˆæ— æ³•è¿æ¥åˆ°huggingface.coï¼‰"""
        try:
            import urllib.request
            import socket

            # å°è¯•è¿æ¥huggingface.coï¼Œè¶…æ—¶æ—¶é—´è®¾ä¸º3ç§’
            socket.setdefaulttimeout(3)
            urllib.request.urlopen('https://huggingface.co', timeout=3)
            return False  # èƒ½è¿æ¥ï¼Œä¸æ˜¯ç¦»çº¿æ¨¡å¼
        except Exception:
            return True   # æ— æ³•è¿æ¥ï¼Œæ˜¯ç¦»çº¿æ¨¡å¼

    def get_text_stats(self) -> Dict[str, any]:
        """
        è·å–æ–‡æœ¬ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«å„ç§ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        if not self.text:
            return {}

        # åŸºç¡€ç»Ÿè®¡
        stats = {
            'å­—ç¬¦æ€»æ•°': len(self.text),
            'å­—ç¬¦æ•°ï¼ˆä¸å«ç©ºæ ¼ï¼‰': len(self.text.replace(' ', '').replace('\n', '').replace('\t', '')),
            'è¡Œæ•°': len(self.text.split('\n')),
            'æ®µè½æ•°': len([p for p in self.text.split('\n\n') if p.strip()]),
        }

        # å¥å­ç»Ÿè®¡
        sentences = self._split_sentences(self.text)
        stats['å¥å­æ•°'] = len([s for s in sentences if s.strip()])

        # è¯æ±‡ç»Ÿè®¡
        try:
            word_freq = self.word_frequency()
            stats['è¯æ±‡æ€»æ•°'] = sum(word_freq.values())
            stats['ä¸é‡å¤è¯æ±‡æ•°'] = len(word_freq)
            if word_freq:
                stats['å¹³å‡è¯é¢‘'] = round(sum(word_freq.values()) / len(word_freq), 2)
                stats['æœ€é«˜è¯é¢‘'] = max(word_freq.values())
        except Exception:
            stats['è¯æ±‡æ€»æ•°'] = 0
            stats['ä¸é‡å¤è¯æ±‡æ•°'] = 0

        # å¹³å‡é•¿åº¦ç»Ÿè®¡
        if stats['å¥å­æ•°'] > 0:
            stats['å¹³å‡å¥å­é•¿åº¦'] = round(stats['å­—ç¬¦æ€»æ•°'] / stats['å¥å­æ•°'], 2)

        return stats
